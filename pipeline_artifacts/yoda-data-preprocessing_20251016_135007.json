{
  "components": {
    "comp-data-transformation-component": {
      "executorLabel": "exec-data-transformation-component",
      "inputDefinitions": {
        "parameters": {
          "input_gcs_path": {
            "description": "GCS path to input CSV file (gs://bucket/path/file.csv)",
            "parameterType": "STRING"
          },
          "output_gcs_bucket": {
            "description": "GCS bucket name for output files",
            "parameterType": "STRING"
          },
          "random_state": {
            "defaultValue": 42.0,
            "description": "Random seed for reproducibility (default: 42)",
            "isOptional": true,
            "parameterType": "NUMBER_INTEGER"
          },
          "test_size": {
            "defaultValue": 0.2,
            "description": "Proportion of test set (default: 0.2)",
            "isOptional": true,
            "parameterType": "NUMBER_DOUBLE"
          },
          "use_extra_translation": {
            "defaultValue": true,
            "description": "Whether to use translation_extra column (default: True)",
            "isOptional": true,
            "parameterType": "BOOLEAN"
          }
        }
      },
      "outputDefinitions": {
        "parameters": {
          "test_examples": {
            "parameterType": "NUMBER_INTEGER"
          },
          "test_output_path": {
            "parameterType": "STRING"
          },
          "train_examples": {
            "parameterType": "NUMBER_INTEGER"
          },
          "train_output_path": {
            "parameterType": "STRING"
          }
        }
      }
    }
  },
  "defaultPipelineRoot": "gs://llmops_101_europ/pipeline_runs",
  "deploymentSpec": {
    "executors": {
      "exec-data-transformation-component": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "data_transformation_component"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'pandas==2.3.3' 'datasets==4.2.0' 'gcsfs==2025.9.0' 'google-cloud-storage==2.19.0' 'python-dotenv==1.1.1'  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.14.6' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef data_transformation_component(\n    input_gcs_path: str,\n    output_gcs_bucket: str,\n    train_output_path: OutputPath(str),\n    test_output_path: OutputPath(str),\n    test_size: float = 0.2,\n    random_state: int = 42,\n    use_extra_translation: bool = True\n) -> NamedTuple(\"DataTransformationOutput\", [(\"train_examples\", int), (\"test_examples\", int)]):\n    \"\"\"\n    Data transformation component for Yoda sentences dataset.\n\n    Args:\n        input_gcs_path (str): GCS path to input CSV file (gs://bucket/path/file.csv)\n        output_gcs_bucket (str): GCS bucket name for output files\n        train_output_path (OutputPath): Path for training dataset output\n        test_output_path (OutputPath): Path for test dataset output\n        test_size (float): Proportion of test set (default: 0.2)\n        random_state (int): Random seed for reproducibility (default: 42)\n        use_extra_translation (bool): Whether to use translation_extra column (default: True)\n\n    Returns:\n        NamedTuple: Contains train_examples and test_examples counts\n    \"\"\"\n    import os\n    import pandas as pd\n    import datasets\n    import logging\n    from google.cloud import storage\n    import json\n    from typing import List, Dict, Any\n\n    # Set up logging\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n    logger = logging.getLogger(__name__)\n\n    logger.info(\"Starting data transformation component\")\n    logger.info(f\"Input GCS path: {input_gcs_path}\")\n    logger.info(f\"Output GCS bucket: {output_gcs_bucket}\")\n    logger.info(f\"Test size: {test_size}\")\n    logger.info(f\"Random state: {random_state}\")\n    logger.info(f\"Use extra translation: {use_extra_translation}\")\n\n    def load_dataset_from_gcs(gcs_path: str) -> pd.DataFrame:\n        \"\"\"Load dataset from GCS.\"\"\"\n        logger.info(f\"Loading dataset from {gcs_path}\")\n        try:\n            df = pd.read_csv(gcs_path)\n            logger.info(f\"Successfully loaded dataset with {len(df)} rows and {len(df.columns)} columns\")\n            logger.info(f\"Columns: {list(df.columns)}\")\n            return df\n        except Exception as e:\n            logger.error(f\"Failed to load dataset from GCS: {str(e)}\")\n            raise\n\n    def format_to_conversational(df: pd.DataFrame, use_extra: bool) -> List[Dict[str, Any]]:\n        \"\"\"Convert to conversational format.\"\"\"\n        logger.info(f\"Converting {len(df)} rows to conversational format\")\n\n        conversations = []\n        translation_column = \"translation_extra\" if use_extra else \"translation\"\n\n        if translation_column not in df.columns:\n            logger.warning(f\"Column '{translation_column}' not found. Using 'translation' instead.\")\n            translation_column = \"translation\"\n\n        for _, row in df.iterrows():\n            conversation = [\n                {\"role\": \"user\", \"content\": row[\"sentence\"]},\n                {\"role\": \"assistant\", \"content\": row[translation_column]}\n            ]\n            conversations.append({\"messages\": conversation})\n\n        logger.info(f\"Successfully created {len(conversations)} conversations\")\n        return conversations\n\n    def upload_to_gcs(local_path: str, bucket_name: str, blob_name: str):\n        \"\"\"Upload file to GCS.\"\"\"\n        logger.info(f\"Uploading {local_path} to gs://{bucket_name}/{blob_name}\")\n        try:\n            client = storage.Client()\n            bucket = client.bucket(bucket_name)\n            blob = bucket.blob(blob_name)\n            blob.upload_from_filename(local_path)\n            logger.info(f\"Successfully uploaded to gs://{bucket_name}/{blob_name}\")\n        except Exception as e:\n            logger.error(f\"Failed to upload to GCS: {str(e)}\")\n            raise\n\n    try:\n        # Step 1: Load dataset from GCS\n        raw_df = load_dataset_from_gcs(input_gcs_path)\n\n        # Step 2: Format to conversational format\n        conversations = format_to_conversational(raw_df, use_extra_translation)\n\n        # Step 3: Convert to Hugging Face Dataset\n        logger.info(\"Converting conversations to Hugging Face Dataset\")\n        df_conversations = pd.DataFrame(conversations)\n        hf_dataset = datasets.Dataset.from_pandas(df_conversations)\n        logger.info(f\"Created Hugging Face dataset with {len(hf_dataset)} examples\")\n\n        # Step 4: Split into train/test\n        logger.info(f\"Splitting dataset with test_size={test_size}, random_state={random_state}\")\n        split_dataset = hf_dataset.train_test_split(test_size=test_size, seed=random_state)\n        train_dataset = split_dataset[\"train\"]\n        test_dataset = split_dataset[\"test\"]\n\n        logger.info(f\"Train set: {len(train_dataset)} examples\")\n        logger.info(f\"Test set: {len(test_dataset)} examples\")\n\n        # Step 5: Save datasets locally (as JSON)\n        logger.info(\"Saving datasets locally\")\n\n        # Create temporary local files\n        import tempfile\n        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:\n            temp_train_path = f.name\n        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:\n            temp_test_path = f.name\n\n        # Save as JSON\n        train_dataset.to_json(temp_train_path)\n        test_dataset.to_json(temp_test_path)\n\n        # Step 6: Copy to output paths (for Kubeflow)\n        import shutil\n        shutil.copy2(temp_train_path, train_output_path)\n        shutil.copy2(temp_test_path, test_output_path)\n\n        logger.info(f\"Saved train dataset to: {train_output_path}\")\n        logger.info(f\"Saved test dataset to: {test_output_path}\")\n\n        # Step 7: Upload to GCS bucket\n        timestamp = pd.Timestamp.now().strftime(\"%Y-%m-%d-%H:%M:%S\")\n        train_gcs_name = f\"processed_data/{timestamp}/train_dataset.json\"\n        test_gcs_name = f\"processed_data/{timestamp}/test_dataset.json\"\n\n        upload_to_gcs(temp_train_path, output_gcs_bucket, train_gcs_name)\n        upload_to_gcs(temp_test_path, output_gcs_bucket, test_gcs_name)\n\n        # Clean up temporary files\n        os.unlink(temp_train_path)\n        os.unlink(temp_test_path)\n\n        logger.info(\"\u2705 Data transformation completed successfully!\")\n\n        # Return the output tuple\n        from collections import namedtuple\n        DataTransformationOutput = namedtuple(\"DataTransformationOutput\", [\"train_examples\", \"test_examples\"])\n        return DataTransformationOutput(train_examples=len(train_dataset), test_examples=len(test_dataset))\n\n    except Exception as e:\n        logger.error(f\"\u274c Data transformation failed: {str(e)}\")\n        raise\n\n"
          ],
          "image": "python:3.11-slim",
          "resources": {
            "cpuLimit": 2.0,
            "cpuRequest": 1.0,
            "memoryLimit": 4.294967296,
            "memoryRequest": 2.147483648,
            "resourceCpuLimit": "2",
            "resourceCpuRequest": "1",
            "resourceMemoryLimit": "4Gi",
            "resourceMemoryRequest": "2Gi"
          }
        }
      }
    }
  },
  "pipelineInfo": {
    "description": "Pipeline to preprocess Yoda sentences dataset for Phi-3 fine-tuning",
    "name": "yoda-data-preprocessing-pipeline"
  },
  "root": {
    "dag": {
      "tasks": {
        "data-transformation-component": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-data-transformation-component"
          },
          "inputs": {
            "parameters": {
              "input_gcs_path": {
                "componentInputParameter": "input_gcs_path"
              },
              "output_gcs_bucket": {
                "componentInputParameter": "output_gcs_bucket"
              },
              "random_state": {
                "componentInputParameter": "random_state"
              },
              "test_size": {
                "componentInputParameter": "test_size"
              },
              "use_extra_translation": {
                "componentInputParameter": "use_extra_translation"
              }
            }
          },
          "taskInfo": {
            "name": "Data Transformation"
          }
        }
      }
    },
    "inputDefinitions": {
      "parameters": {
        "input_gcs_path": {
          "defaultValue": "gs://llmops_101_europ/15-10-2025-08:50:00/yoda_sentences.csv",
          "description": "GCS path to input CSV file",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "output_gcs_bucket": {
          "defaultValue": "llmops_101_europ",
          "description": "GCS bucket name for output files",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "random_state": {
          "defaultValue": 42.0,
          "description": "Random seed for reproducibility (default: 42)",
          "isOptional": true,
          "parameterType": "NUMBER_INTEGER"
        },
        "test_size": {
          "defaultValue": 0.2,
          "description": "Proportion of test set (default: 0.2)",
          "isOptional": true,
          "parameterType": "NUMBER_DOUBLE"
        },
        "use_extra_translation": {
          "defaultValue": true,
          "description": "Whether to use translation_extra column (default: True)",
          "isOptional": true,
          "parameterType": "BOOLEAN"
        }
      }
    }
  },
  "schemaVersion": "2.1.0",
  "sdkVersion": "kfp-2.14.6"
}