{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d27222ca",
   "metadata": {},
   "source": [
    "# ü•ó Nutrition Assistant - Complete MLOps Pipeline\n",
    "\n",
    "**Built for:** Albert School LLM OPS Bootcamp MSC2  \n",
    "**Status:** ‚úÖ Production Ready  \n",
    "**Date Completed:** October 21, 2025  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Table of Contents\n",
    "\n",
    "1. [Project Overview & Architecture](#1)\n",
    "2. [Environment Setup & Prerequisites](#2)\n",
    "3. [GCP Configuration & Authentication](#3)\n",
    "4. [Data Processing Pipeline](#4)\n",
    "5. [Model Training with Vertex AI](#5)\n",
    "6. [LoRA Fine-Tuning Configuration](#6)\n",
    "7. [Model Deployment to Vertex AI](#7)\n",
    "8. [Custom Handler Implementation](#8)\n",
    "9. [Chainlit Chatbot Interface](#9)\n",
    "10. [Cost Management & Monitoring](#10)\n",
    "11. [Pipeline Components Deep Dive](#11)\n",
    "12. [Troubleshooting Guide](#12)\n",
    "\n",
    "---\n",
    "\n",
    "## What This Project Does\n",
    "\n",
    "This is an **end-to-end MLOps system** that:\n",
    "- ‚úÖ **Transforms** 2,395 nutrition items into conversational format\n",
    "- ‚úÖ **Fine-tunes** Microsoft Phi-3-mini model using LoRA (Low-Rank Adaptation)\n",
    "- ‚úÖ **Deploys** to Vertex AI endpoint with GPU acceleration\n",
    "- ‚úÖ **Serves** via a beautiful Chainlit web interface\n",
    "- ‚úÖ **Manages costs** with deploy/undeploy automation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eec678",
   "metadata": {},
   "source": [
    "# 1. Project Overview & Architecture üèóÔ∏è\n",
    "\n",
    "## Complete System Architecture\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                         DATA PROCESSING                                  ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
    "‚îÇ  ‚îÇ COMBINED_FOOD_DATASET.csv (2,395 food items)                       ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ ‚Üí Transform to conversational Q&A format                           ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ ‚Üí Train/Test split                                                 ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ ‚Üí Upload to GCS: gs://llmops_101_europ/data/                      ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                    ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    VERTEX AI TRAINING PIPELINE                           ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ\n",
    "‚îÇ  ‚îÇ Component 1  ‚îÇ ‚Üí ‚îÇ Component 2  ‚îÇ ‚Üí ‚îÇ Component 3  ‚îÇ‚Üí ‚îÇComponent 4‚îÇ‚îÇ\n",
    "‚îÇ  ‚îÇ Transform    ‚îÇ   ‚îÇ Fine-Tune    ‚îÇ   ‚îÇ Inference    ‚îÇ  ‚îÇ Evaluate  ‚îÇ‚îÇ\n",
    "‚îÇ  ‚îÇ              ‚îÇ   ‚îÇ LoRA + 4-bit ‚îÇ   ‚îÇ Generate     ‚îÇ  ‚îÇ BLEU/     ‚îÇ‚îÇ\n",
    "‚îÇ  ‚îÇ CSV ‚Üí JSON   ‚îÇ   ‚îÇ Phi-3 model  ‚îÇ   ‚îÇ Predictions  ‚îÇ  ‚îÇ Rouge     ‚îÇ‚îÇ\n",
    "‚îÇ  ‚îÇ              ‚îÇ   ‚îÇ Tesla T4 GPU ‚îÇ   ‚îÇ              ‚îÇ  ‚îÇ           ‚îÇ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ\n",
    "‚îÇ                                                                          ‚îÇ\n",
    "‚îÇ  Output: gs://llmops_101_europ/pipeline_root/.../fine_tuned_model/     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                    ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                      MODEL DEPLOYMENT                                    ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
    "‚îÇ  ‚îÇ 1. Register Model ‚Üí Vertex AI Model Registry                      ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ    Model ID: 3561348948692041728                                  ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ                                                                     ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ 2. Create Endpoint ‚Üí nutrition-assistant-endpoint                 ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ    Endpoint ID: 5724492940806455296                               ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ                                                                     ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ 3. Deploy Model ‚Üí n1-standard-8 + Tesla T4 GPU                   ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ    Custom Handler: src/handler.py                                 ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ    Container: HuggingFace TGI PyTorch                            ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                    ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                      CHAINLIT WEB INTERFACE                              ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
    "‚îÇ  ‚îÇ User ‚Üí Chat UI (localhost:8000)                                   ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚Üì                                                                ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ Chainlit App (src/app/main.py)                                    ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚Üì                                                                ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ Google Cloud ADC Authentication                                   ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚Üì                                                                ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ Vertex AI Endpoint Prediction Request                            ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚Üì                                                                ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ Model Response (nutrition advice)                                 ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "## Key Technologies\n",
    "\n",
    "| Component | Technology | Purpose |\n",
    "|-----------|------------|---------|\n",
    "| **Model** | Microsoft Phi-3-mini-4k-instruct (3.8B params) | Base LLM |\n",
    "| **Fine-Tuning** | LoRA (Low-Rank Adaptation) + 4-bit quantization | Efficient training |\n",
    "| **Pipeline** | Vertex AI + Kubeflow v2.14.6 | Orchestration |\n",
    "| **Compute** | Tesla T4 GPU + n1-standard-8 | Training & serving |\n",
    "| **Storage** | Google Cloud Storage | Data & models |\n",
    "| **Interface** | Chainlit + Python | User chat UI |\n",
    "| **Auth** | Google Cloud ADC | Security |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d418e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load project configuration\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from src.constants import (\n",
    "    GCP_PROJECT_ID, \n",
    "    GCP_REGION, \n",
    "    GCP_BUCKET_NAME,\n",
    "    BASE_MODEL,\n",
    "    TRAINING_CONFIG,\n",
    "    LORA_CONFIG\n",
    ")\n",
    "\n",
    "print(\"üìä PROJECT CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"GCP Project ID:    {GCP_PROJECT_ID}\")\n",
    "print(f\"GCP Region:        {GCP_REGION}\")\n",
    "print(f\"GCS Bucket:        {GCP_BUCKET_NAME}\")\n",
    "print(f\"Base Model:        {BASE_MODEL}\")\n",
    "print(f\"Training Epochs:   {TRAINING_CONFIG['num_train_epochs']}\")\n",
    "print(f\"Batch Size:        {TRAINING_CONFIG['per_device_train_batch_size']}\")\n",
    "print(f\"Learning Rate:     {TRAINING_CONFIG['learning_rate']}\")\n",
    "print(f\"LoRA Rank (r):     {LORA_CONFIG['r']}\")\n",
    "print(f\"LoRA Alpha:        {LORA_CONFIG['lora_alpha']}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ce702c",
   "metadata": {},
   "source": [
    "# 2. Environment Setup & Prerequisites üîß\n",
    "\n",
    "## Required Software\n",
    "\n",
    "- **Python:** 3.11.6+\n",
    "- **Package Manager:** `uv` (ultra-fast Python package installer)\n",
    "- **Google Cloud SDK:** `gcloud` CLI\n",
    "- **GPU Quota:** NVIDIA Tesla T4 in europe-west2\n",
    "\n",
    "## Environment Validation\n",
    "\n",
    "Run the following checks to ensure your environment is ready:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b2bd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Python version\n",
    "import sys\n",
    "print(f\"‚úÖ Python Version: {sys.version}\")\n",
    "\n",
    "# Check installed packages\n",
    "import subprocess\n",
    "\n",
    "packages_to_check = ['google-cloud-aiplatform', 'chainlit', 'transformers', 'peft', 'bitsandbytes']\n",
    "print(\"\\nüì¶ Installed Packages:\")\n",
    "for package in packages_to_check:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "        print(f\"  ‚úÖ {package}\")\n",
    "    except ImportError:\n",
    "        print(f\"  ‚ùå {package} - NOT INSTALLED\")\n",
    "\n",
    "# Check gcloud CLI\n",
    "try:\n",
    "    result = subprocess.run(['gcloud', 'version'], capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"\\n‚úÖ gcloud CLI installed\")\n",
    "        print(result.stdout.split('\\n')[0])\n",
    "    else:\n",
    "        print(\"\\n‚ùå gcloud CLI not found\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå gcloud CLI not found: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e48c703",
   "metadata": {},
   "source": [
    "# 3. GCP Configuration & Authentication üîê\n",
    "\n",
    "## Authentication Process\n",
    "\n",
    "To work with Vertex AI and Google Cloud Storage, you need proper authentication:\n",
    "\n",
    "### Method 1: Application Default Credentials (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54e9570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Google Cloud authentication\n",
    "import google.auth\n",
    "from google.cloud import aiplatform, storage\n",
    "\n",
    "try:\n",
    "    # Get default credentials\n",
    "    credentials, project = google.auth.default()\n",
    "    print(f\"‚úÖ Authentication successful!\")\n",
    "    print(f\"üìã Project ID: {project}\")\n",
    "    print(f\"üë§ Credentials type: {type(credentials).__name__}\")\n",
    "    \n",
    "    # Initialize Vertex AI\n",
    "    aiplatform.init(\n",
    "        project='aerobic-polygon-460910-v9',\n",
    "        location='europe-west2'\n",
    "    )\n",
    "    print(f\"‚úÖ Vertex AI initialized\")\n",
    "    \n",
    "    # Test GCS access\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket('llmops_101_europ')\n",
    "    if bucket.exists():\n",
    "        print(f\"‚úÖ GCS Bucket accessible: gs://llmops_101_europ\")\n",
    "    else:\n",
    "        print(f\"‚ùå GCS Bucket not found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Authentication failed: {e}\")\n",
    "    print(\"\\nüí° Run this in terminal:\")\n",
    "    print(\"   gcloud auth application-default login\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f0f88e",
   "metadata": {},
   "source": [
    "# 4. Data Processing Pipeline üìä\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "The dataset consists of 2,395 nutrition items from `COMBINED_FOOD_DATASET.csv`.\n",
    "\n",
    "### Data Transformation Process\n",
    "\n",
    "```\n",
    "Raw CSV ‚Üí Parse nutritional data ‚Üí Create Q&A pairs ‚Üí JSON Lines format\n",
    "```\n",
    "\n",
    "Each food item is converted into a conversation:\n",
    "- **User message:** Question about the food\n",
    "- **Assistant message:** Nutritional information response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4048169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore the dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the CSV dataset\n",
    "df = pd.read_csv('COMBINED_FOOD_DATASET.csv')\n",
    "\n",
    "print(f\"üìä Dataset Statistics:\")\n",
    "print(f\"   Total items: {len(df)}\")\n",
    "print(f\"   Columns: {list(df.columns)}\")\n",
    "print(f\"\\nüìã Sample data:\")\n",
    "print(df.head(3))\n",
    "\n",
    "# Show how data is transformed\n",
    "print(\"\\n\\nüîÑ Example Transformation:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load a sample processed conversation\n",
    "try:\n",
    "    with open('data/processed/sample_nutrition_conversations.json', 'r') as f:\n",
    "        sample = json.load(f)\n",
    "    \n",
    "    if isinstance(sample, list) and len(sample) > 0:\n",
    "        example = sample[0]\n",
    "        print(\"INPUT (from CSV):\")\n",
    "        print(f\"  Food: {example.get('food_name', 'N/A')}\")\n",
    "        print(\"\\nOUTPUT (conversational format):\")\n",
    "        print(json.dumps(example.get('messages', []), indent=2))\n",
    "except FileNotFoundError:\n",
    "    print(\"Sample file not found. Run the pipeline to generate it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de00399",
   "metadata": {},
   "source": [
    "# 5. Model Training with Vertex AI üöÄ\n",
    "\n",
    "## Pipeline Components\n",
    "\n",
    "The training pipeline consists of 4 components:\n",
    "\n",
    "### Component 1: Data Transformation\n",
    "- **Input:** CSV file from GCS\n",
    "- **Process:** Convert to conversational Q&A format\n",
    "- **Output:** JSON Lines file split into train/test\n",
    "- **Resources:** 4 CPUs, 8GB RAM\n",
    "\n",
    "### Component 2: Fine-Tuning\n",
    "- **Input:** Training data (JSON Lines)\n",
    "- **Process:** Fine-tune Phi-3-mini with LoRA + 4-bit quantization\n",
    "- **Output:** Fine-tuned model saved to GCS\n",
    "- **Resources:** Tesla T4 GPU, 16 CPUs, 50GB RAM\n",
    "- **Duration:** ~30-45 minutes\n",
    "\n",
    "### Component 3: Inference\n",
    "- **Input:** Test data + fine-tuned model\n",
    "- **Process:** Generate predictions on test set\n",
    "- **Output:** Predictions CSV\n",
    "- **Resources:** Tesla T4 GPU, 8 CPUs, 32GB RAM\n",
    "\n",
    "### Component 4: Evaluation\n",
    "- **Input:** Predictions + ground truth\n",
    "- **Process:** Compute BLEU and Rouge scores\n",
    "- **Output:** Metrics JSON\n",
    "- **Resources:** 4 CPUs, 8GB RAM\n",
    "\n",
    "## Pipeline Execution Flow\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Data Transform] --> B[Fine-Tune]\n",
    "    B --> C[Inference]\n",
    "    C --> D[Evaluation]\n",
    "    B -.Model.-> C\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0980fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check latest pipeline run\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project='aerobic-polygon-460910-v9', location='europe-west2')\n",
    "\n",
    "# List recent pipeline runs\n",
    "print(\"üìã Recent Pipeline Runs:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    pipeline_jobs = aiplatform.PipelineJob.list(\n",
    "        filter='display_name:\"nutrition*\"',\n",
    "        order_by='create_time desc'\n",
    "    )\n",
    "    \n",
    "    for i, job in enumerate(list(pipeline_jobs)[:3]):  # Show last 3\n",
    "        print(f\"\\n{i+1}. {job.display_name}\")\n",
    "        print(f\"   State: {job.state.name}\")\n",
    "        print(f\"   Created: {job.create_time}\")\n",
    "        print(f\"   Job ID: {job.name.split('/')[-1]}\")\n",
    "        \n",
    "        if job.state.name == 'PIPELINE_STATE_SUCCEEDED':\n",
    "            print(f\"   ‚úÖ Status: SUCCEEDED\")\n",
    "        elif job.state.name == 'PIPELINE_STATE_RUNNING':\n",
    "            print(f\"   üîÑ Status: RUNNING\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Status: {job.state.name}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Could not fetch pipeline runs: {e}\")\n",
    "    print(\"\\nüí° Run the pipeline first:\")\n",
    "    print(\"   python scripts/pipeline_runner.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4696c553",
   "metadata": {},
   "source": [
    "# 6. LoRA Fine-Tuning Configuration ‚öôÔ∏è\n",
    "\n",
    "## What is LoRA?\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)** is a parameter-efficient fine-tuning method that:\n",
    "- Freezes the base model weights\n",
    "- Adds small trainable matrices to specific layers\n",
    "- Reduces trainable parameters by ~99.8%\n",
    "- Enables fine-tuning on consumer GPUs\n",
    "\n",
    "## Our LoRA Configuration\n",
    "\n",
    "| Parameter | Value | Purpose |\n",
    "|-----------|-------|---------|\n",
    "| **r (rank)** | 16 | Dimension of low-rank matrices |\n",
    "| **lora_alpha** | 32 | Scaling factor (typically 2√ór) |\n",
    "| **lora_dropout** | 0.05 | Dropout for regularization |\n",
    "| **Target Modules** | q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj | Which layers to adapt |\n",
    "\n",
    "## Why 4-bit Quantization?\n",
    "\n",
    "- **Reduces GPU memory:** 3.8B params √ó 4 bits = ~15GB (vs 60GB for FP16)\n",
    "- **Enables training on T4 GPU:** 16GB VRAM\n",
    "- **Uses NF4 (Normalized Float 4):** Optimal for neural network weights\n",
    "- **Minimal accuracy loss:** <1% with proper configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ee1784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate trainable parameters\n",
    "from src.constants import LORA_CONFIG\n",
    "\n",
    "r = LORA_CONFIG['r']\n",
    "lora_alpha = LORA_CONFIG['lora_alpha']\n",
    "\n",
    "# Phi-3-mini has ~3.8B parameters\n",
    "total_params = 3_800_000_000\n",
    "\n",
    "# Estimate LoRA parameters (simplified calculation)\n",
    "# For each target module: 2 matrices of size (hidden_dim, r)\n",
    "# Phi-3 has hidden_dim=3072, and we target 7 modules per layer across ~32 layers\n",
    "hidden_dim = 3072\n",
    "num_modules_per_layer = 7\n",
    "num_layers = 32\n",
    "\n",
    "lora_params = 2 * hidden_dim * r * num_modules_per_layer * num_layers\n",
    "\n",
    "print(\"üìä LoRA Parameter Analysis\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total Model Parameters:     {total_params:,}\")\n",
    "print(f\"LoRA Trainable Parameters:  {lora_params:,}\")\n",
    "print(f\"Percentage Trainable:       {(lora_params/total_params)*100:.2f}%\")\n",
    "print(f\"\\nüíæ Memory Savings:\")\n",
    "print(f\"Full Fine-tuning:           ~60 GB (FP16)\")\n",
    "print(f\"LoRA + 4-bit:               ~15 GB\")\n",
    "print(f\"Reduction:                  75%\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410a8e66",
   "metadata": {},
   "source": [
    "# 7. Model Deployment to Vertex AI üö¢\n",
    "\n",
    "## Deployment Architecture\n",
    "\n",
    "```\n",
    "Model (GCS) ‚Üí Register to Model Registry ‚Üí Deploy to Endpoint ‚Üí Serve Predictions\n",
    "```\n",
    "\n",
    "## Deployment Configuration\n",
    "\n",
    "| Setting | Value | Purpose |\n",
    "|---------|-------|---------|\n",
    "| **Model ID** | 3561348948692041728 | Registered model in Vertex AI |\n",
    "| **Endpoint ID** | 5724492940806455296 | Production endpoint |\n",
    "| **Machine Type** | n1-standard-8 | 8 vCPUs, 30GB RAM |\n",
    "| **Accelerator** | NVIDIA Tesla T4 | GPU for inference |\n",
    "| **Container** | HuggingFace TGI PyTorch | Pre-built serving container |\n",
    "| **Min Replicas** | 1 | Always-on instance |\n",
    "| **Max Replicas** | 1 | No auto-scaling (cost control) |\n",
    "\n",
    "## Deployment Process\n",
    "\n",
    "### Step 1: Register Model\n",
    "The model artifact from the training pipeline is registered to Vertex AI Model Registry with a custom prediction handler.\n",
    "\n",
    "### Step 2: Create/Get Endpoint\n",
    "An endpoint is created (or retrieved if it exists) to serve predictions.\n",
    "\n",
    "### Step 3: Deploy Model\n",
    "The model is deployed to the endpoint with GPU configuration.\n",
    "\n",
    "**‚è±Ô∏è Deployment time:** 5-10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c60fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check endpoint status\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project='aerobic-polygon-460910-v9', location='europe-west2')\n",
    "\n",
    "ENDPOINT_ID = '5724492940806455296'\n",
    "\n",
    "try:\n",
    "    endpoint = aiplatform.Endpoint(f\"projects/aerobic-polygon-460910-v9/locations/europe-west2/endpoints/{ENDPOINT_ID}\")\n",
    "    \n",
    "    print(\"üìç ENDPOINT STATUS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Endpoint ID:       {ENDPOINT_ID}\")\n",
    "    print(f\"Display Name:      {endpoint.display_name}\")\n",
    "    print(f\"Resource Name:     {endpoint.resource_name}\")\n",
    "    print(f\"Create Time:       {endpoint.create_time}\")\n",
    "    \n",
    "    # Check deployed models\n",
    "    deployed_models = endpoint.list_models()\n",
    "    \n",
    "    if deployed_models:\n",
    "        print(f\"\\n‚úÖ Models Deployed: {len(deployed_models)}\")\n",
    "        for model in deployed_models:\n",
    "            print(f\"\\n   Model ID: {model.id}\")\n",
    "            print(f\"   Display Name: {model.display_name}\")\n",
    "            print(f\"   Machine Type: {model.machine_spec.machine_type}\")\n",
    "            print(f\"   Accelerator: {model.machine_spec.accelerator_type}\")\n",
    "            print(f\"   Accelerator Count: {model.machine_spec.accelerator_count}\")\n",
    "            print(f\"   Traffic: {model.traffic_percentage}%\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö™ No models currently deployed\")\n",
    "        print(f\"üí∞ Current cost: $0/hour\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error checking endpoint: {e}\")\n",
    "    print(\"\\nüí° Endpoint may not exist yet. Run:\")\n",
    "    print(\"   python scripts/deploy_to_endpoint.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c778b21e",
   "metadata": {},
   "source": [
    "# 8. üöÄ Launcher - Start/Stop Your Chatbot\n",
    "\n",
    "## ‚ñ∂Ô∏è START THE CHATBOT (3 Steps)\n",
    "\n",
    "### Step 1: Deploy Model to Endpoint\n",
    "\n",
    "```powershell\n",
    "python scripts/deploy_to_endpoint.py\n",
    "```\n",
    "\n",
    "‚è±Ô∏è **Wait:** 5-10 minutes for deployment to complete\n",
    "\n",
    "### Step 2: Verify Deployment\n",
    "\n",
    "```powershell\n",
    "python scripts/check_endpoint_status.py\n",
    "```\n",
    "\n",
    "‚úÖ **Look for:** \"DEPLOYMENT COMPLETE!\" and Status: \"SERVING\"\n",
    "\n",
    "### Step 3: Launch Web Interface\n",
    "\n",
    "```powershell\n",
    ".\\.venv\\Scripts\\chainlit.exe run src/app/main.py -w\n",
    "```\n",
    "\n",
    "üåê **Opens at:** http://localhost:8000\n",
    "\n",
    "---\n",
    "\n",
    "## ‚èπÔ∏è STOP EVERYTHING (Save Money!)\n",
    "\n",
    "### Step 1: Stop Chatbot Interface\n",
    "\n",
    "Press `Ctrl+C` in the terminal running Chainlit\n",
    "\n",
    "### Step 2: Undeploy Model (STOPS BILLING!)\n",
    "\n",
    "```powershell\n",
    "python scripts/undeploy_model.py\n",
    "```\n",
    "\n",
    "üí∞ **This immediately stops hourly charges!**\n",
    "\n",
    "### Step 3: Verify Model is Undeployed\n",
    "\n",
    "```powershell\n",
    "python scripts/check_endpoint_status.py\n",
    "```\n",
    "\n",
    "‚úÖ **Should show:** \"Status: No models deployed\"\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Monitoring Links\n",
    "\n",
    "- **Endpoint Status:** [View in GCP Console](https://console.cloud.google.com/vertex-ai/online-prediction/endpoints/5724492940806455296?project=aerobic-polygon-460910-v9)\n",
    "- **Pipeline History:** [View Training Runs](https://console.cloud.google.com/vertex-ai/pipelines?project=aerobic-polygon-460910-v9)\n",
    "- **Storage Bucket:** [View GCS Files](https://console.cloud.google.com/storage/browser/llmops_101_europ?project=aerobic-polygon-460910-v9)\n",
    "\n",
    "---\n",
    "\n",
    "## üñ±Ô∏è Manual Deployment via GCP Console\n",
    "\n",
    "If you prefer to deploy through the web interface:\n",
    "\n",
    "### Settings to Use:\n",
    "\n",
    "| Setting | Value |\n",
    "|---------|-------|\n",
    "| **Model Name** | nutrition-assistant-phi3 |\n",
    "| **Model ID** | 3561348948692041728 |\n",
    "| **Deployed Name** | nutrition-assistant-deployed |\n",
    "| **Machine Type** | n1-standard-8 |\n",
    "| **Accelerator** | NVIDIA_TESLA_T4 |\n",
    "| **Accelerator Count** | 1 |\n",
    "| **Min Nodes** | 1 |\n",
    "| **Max Nodes** | 1 |\n",
    "| **Traffic Split** | 100 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb527ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can run these commands directly from the notebook (not recommended - use terminal instead)\n",
    "# This cell demonstrates what happens when you check status\n",
    "\n",
    "import subprocess\n",
    "\n",
    "print(\"üîç Checking Endpoint Status...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        ['python', 'scripts/check_endpoint_status.py'],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=30\n",
    "    )\n",
    "    \n",
    "    print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print(\"Errors:\", result.stderr)\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"‚è±Ô∏è Command timed out\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    \n",
    "print(\"\\nüí° TIP: Run these commands in your terminal for better output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2210e458",
   "metadata": {},
   "source": [
    "# 9. Custom Handler Implementation üõ†Ô∏è\n",
    "\n",
    "The custom handler (`src/handler.py`) is crucial for deployment. It tells Vertex AI how to:\n",
    "1. Load the fine-tuned model\n",
    "2. Process prediction requests\n",
    "3. Return formatted responses\n",
    "\n",
    "## Handler Structure\n",
    "\n",
    "```python\n",
    "class EndpointHandler:\n",
    "    def __init__(self, model_dir):\n",
    "        # Load the fine-tuned model with LoRA adapters\n",
    "        # Apply 4-bit quantization\n",
    "        # Load tokenizer\n",
    "        \n",
    "    def __call__(self, request):\n",
    "        # Extract input text\n",
    "        # Tokenize\n",
    "        # Generate response\n",
    "        # Return JSON\n",
    "```\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **4-bit Quantization:** Loads model in NF4 format to fit in GPU memory\n",
    "- **LoRA Adapters:** Applies fine-tuned weights on top of base model\n",
    "- **Streaming Support:** Can return responses token-by-token\n",
    "- **Error Handling:** Graceful fallbacks for invalid inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3fa2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the handler implementation\n",
    "with open('src/handler.py', 'r') as f:\n",
    "    handler_code = f.read()\n",
    "    \n",
    "print(\"üìÑ Custom Handler Code (src/handler.py)\")\n",
    "print(\"=\" * 70)\n",
    "print(handler_code[:1500])  # Show first 1500 characters\n",
    "print(\"\\n... (truncated)\")\n",
    "print(\"\\nüí° Full file: src/handler.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da354af6",
   "metadata": {},
   "source": [
    "# 10. Cost Management & Monitoring üí∞\n",
    "\n",
    "## Cost Breakdown\n",
    "\n",
    "| Status | Component | Cost/Hour | Cost/Day (8hrs) | Cost/Month |\n",
    "|--------|-----------|-----------|-----------------|------------|\n",
    "| ‚ö™ **Undeployed** | Endpoint (empty) | $0.00 | $0.00 | $0.00 |\n",
    "| ‚ö™ **Undeployed** | GCS Storage | ~$0.001 | ~$0.008 | ~$0.72 |\n",
    "| | **TOTAL UNDEPLOYED** | **$0.001** | **$0.008** | **~$0.72** |\n",
    "| | | | | |\n",
    "| ‚úÖ **Deployed** | n1-standard-8 | ~$0.38 | ~$3.04 | ~$274 |\n",
    "| ‚úÖ **Deployed** | Tesla T4 GPU | ~$0.35 | ~$2.80 | ~$252 |\n",
    "| ‚úÖ **Deployed** | Endpoint overhead | ~$0.02 | ~$0.16 | ~$14 |\n",
    "| | **TOTAL DEPLOYED** | **~$0.75** | **~$6.00** | **~$540** |\n",
    "\n",
    "## Cost Optimization Best Practices\n",
    "\n",
    "1. **‚úÖ Always undeploy after testing**\n",
    "   ```powershell\n",
    "   python scripts/undeploy_model.py\n",
    "   ```\n",
    "\n",
    "2. **‚úÖ Check status before leaving**\n",
    "   ```powershell\n",
    "   python scripts/check_endpoint_status.py\n",
    "   ```\n",
    "\n",
    "3. **‚úÖ Set up billing alerts in GCP Console**\n",
    "   - Go to: Billing ‚Üí Budgets & alerts\n",
    "   - Set threshold: $10/month\n",
    "   - Get email notifications\n",
    "\n",
    "4. **‚ùå Don't leave models deployed overnight**\n",
    "   - 8 hours unused = ~$6 wasted\n",
    "   - 1 month unused = ~$540!\n",
    "\n",
    "## Monitoring Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb73801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate estimated costs based on current deployment status\n",
    "from google.cloud import aiplatform\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "aiplatform.init(project='aerobic-polygon-460910-v9', location='europe-west2')\n",
    "\n",
    "ENDPOINT_ID = '5724492940806455296'\n",
    "\n",
    "try:\n",
    "    endpoint = aiplatform.Endpoint(f\"projects/aerobic-polygon-460910-v9/locations/europe-west2/endpoints/{ENDPOINT_ID}\")\n",
    "    deployed_models = endpoint.list_models()\n",
    "    \n",
    "    print(\"üí∞ CURRENT COST ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if deployed_models:\n",
    "        print(\"‚ö†Ô∏è  MODEL IS DEPLOYED - BILLING ACTIVE\")\n",
    "        print(\"\\nHourly Cost Breakdown:\")\n",
    "        print(\"  n1-standard-8 (8 vCPUs, 30GB RAM): $0.38/hour\")\n",
    "        print(\"  Tesla T4 GPU:                      $0.35/hour\")\n",
    "        print(\"  Endpoint overhead:                 $0.02/hour\")\n",
    "        print(\"  \" + \"-\" * 50)\n",
    "        print(\"  TOTAL:                             $0.75/hour\")\n",
    "        \n",
    "        # Estimate time deployed\n",
    "        create_time = endpoint.create_time\n",
    "        if create_time:\n",
    "            hours_deployed = (datetime.now(create_time.tzinfo) - create_time).total_seconds() / 3600\n",
    "            estimated_cost = hours_deployed * 0.75\n",
    "            print(f\"\\nEstimated cost since creation:     ${estimated_cost:.2f}\")\n",
    "            \n",
    "        print(\"\\n‚ö†Ô∏è  ACTION REQUIRED: Run `python scripts/undeploy_model.py` to stop billing!\")\n",
    "    else:\n",
    "        print(\"‚úÖ MODEL IS UNDEPLOYED - MINIMAL BILLING\")\n",
    "        print(\"\\nCurrent costs:\")\n",
    "        print(\"  GCS Storage only:                  $0.001/hour\")\n",
    "        print(\"  Daily cost:                        $0.024\")\n",
    "        print(\"  Monthly cost:                      ~$0.72\")\n",
    "        print(\"\\n‚úÖ You're good! No action needed.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Could not check costs: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990c6fb4",
   "metadata": {},
   "source": [
    "# 11. Pipeline Components Deep Dive üî¨\n",
    "\n",
    "## Component Files\n",
    "\n",
    "All pipeline components are in `src/pipeline_components/`:\n",
    "\n",
    "### 1. data_transformation_component.py\n",
    "**Purpose:** Transform CSV to conversational format\n",
    "\n",
    "**Key Functions:**\n",
    "- `transform_data()`: Main transformation logic\n",
    "- `create_conversation()`: Converts food item to Q&A pair\n",
    "- `split_train_test()`: 80/20 split\n",
    "\n",
    "**Inputs:**\n",
    "- `dataset_path`: Path to CSV in GCS\n",
    "\n",
    "**Outputs:**\n",
    "- `train_data`: Training JSON Lines\n",
    "- `test_data`: Test JSON Lines\n",
    "\n",
    "**Resources:**\n",
    "- CPU: 4\n",
    "- RAM: 8GB\n",
    "- Disk: 10GB\n",
    "\n",
    "---\n",
    "\n",
    "### 2. fine_tuning_component.py\n",
    "**Purpose:** Fine-tune Phi-3 with LoRA\n",
    "\n",
    "**Key Functions:**\n",
    "- `load_model()`: Load base model with 4-bit quantization\n",
    "- `apply_lora()`: Add LoRA adapters\n",
    "- `train()`: Training loop with Transformers Trainer\n",
    "\n",
    "**Inputs:**\n",
    "- `train_data`: Training dataset\n",
    "- `base_model_id`: \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "**Outputs:**\n",
    "- `model_dir`: Fine-tuned model in GCS\n",
    "\n",
    "**Resources:**\n",
    "- GPU: Tesla T4 (16GB VRAM)\n",
    "- CPU: 16\n",
    "- RAM: 50GB\n",
    "- Duration: ~30-45 minutes\n",
    "\n",
    "---\n",
    "\n",
    "### 3. inference_component.py\n",
    "**Purpose:** Generate predictions on test set\n",
    "\n",
    "**Key Functions:**\n",
    "- `load_model()`: Load fine-tuned model\n",
    "- `predict()`: Generate responses\n",
    "- `batch_predict()`: Process multiple inputs\n",
    "\n",
    "**Inputs:**\n",
    "- `model_dir`: Fine-tuned model\n",
    "- `test_data`: Test dataset\n",
    "\n",
    "**Outputs:**\n",
    "- `predictions`: CSV with generated responses\n",
    "\n",
    "**Resources:**\n",
    "- GPU: Tesla T4\n",
    "- CPU: 8\n",
    "- RAM: 32GB\n",
    "\n",
    "---\n",
    "\n",
    "### 4. evaluation_component.py\n",
    "**Purpose:** Compute metrics (BLEU, Rouge)\n",
    "\n",
    "**Key Functions:**\n",
    "- `compute_bleu()`: BLEU score calculation\n",
    "- `compute_rouge()`: Rouge score calculation\n",
    "- `aggregate_metrics()`: Summary statistics\n",
    "\n",
    "**Inputs:**\n",
    "- `predictions`: Model predictions\n",
    "- `ground_truth`: Reference answers\n",
    "\n",
    "**Outputs:**\n",
    "- `metrics`: JSON with scores\n",
    "- `per_sample_metrics`: Detailed results\n",
    "\n",
    "**Resources:**\n",
    "- CPU: 4\n",
    "- RAM: 8GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c174187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all component files\n",
    "import os\n",
    "\n",
    "components_dir = 'src/pipeline_components'\n",
    "print(\"üìÅ Pipeline Component Files:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for filename in os.listdir(components_dir):\n",
    "    if filename.endswith('.py') and not filename.startswith('__'):\n",
    "        filepath = os.path.join(components_dir, filename)\n",
    "        # Get file size\n",
    "        size = os.path.getsize(filepath)\n",
    "        print(f\"\\n‚úÖ {filename}\")\n",
    "        print(f\"   Size: {size:,} bytes ({size//1024} KB)\")\n",
    "        \n",
    "        # Count lines\n",
    "        with open(filepath, 'r') as f:\n",
    "            lines = len(f.readlines())\n",
    "        print(f\"   Lines of code: {lines}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üí° View full code in: src/pipeline_components/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9081389f",
   "metadata": {},
   "source": [
    "# 12. Troubleshooting Guide üîß\n",
    "\n",
    "## Common Issues and Solutions\n",
    "\n",
    "### ‚ùå Issue 1: Authentication Error\n",
    "\n",
    "**Symptom:**\n",
    "```\n",
    "Error: Could not automatically determine credentials\n",
    "```\n",
    "\n",
    "**Solution:**\n",
    "```powershell\n",
    "gcloud auth application-default login\n",
    "```\n",
    "\n",
    "**Why it happens:** Your local credentials expired or weren't set up\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Issue 2: GPU Quota Error\n",
    "\n",
    "**Symptom:**\n",
    "```\n",
    "Quota 'NVIDIA_T4_GPUS' exceeded\n",
    "```\n",
    "\n",
    "**Solution:**\n",
    "1. Go to: [GCP Console ‚Üí Quotas](https://console.cloud.google.com/iam-admin/quotas)\n",
    "2. Filter: \"Vertex AI\" + \"europe-west2\" + \"NVIDIA T4\"\n",
    "3. Request quota increase to 1\n",
    "4. Wait for approval (usually 1-2 hours)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Issue 3: Model Not Responding\n",
    "\n",
    "**Symptom:**\n",
    "Chainlit shows errors or no response\n",
    "\n",
    "**Solution:**\n",
    "```powershell\n",
    "# 1. Check endpoint status\n",
    "python scripts/check_endpoint_status.py\n",
    "\n",
    "# 2. If not SERVING, check deployment\n",
    "# Look for errors in GCP Console\n",
    "\n",
    "# 3. If needed, redeploy\n",
    "python scripts/undeploy_model.py\n",
    "python scripts/deploy_to_endpoint.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Issue 4: Pipeline Fails at Fine-Tuning\n",
    "\n",
    "**Symptom:**\n",
    "Pipeline shows \"FAILED\" status on fine-tuning component\n",
    "\n",
    "**Checklist:**\n",
    "- ‚úÖ GPU quota approved?\n",
    "- ‚úÖ Dataset uploaded to GCS?\n",
    "- ‚úÖ Correct region (europe-west2)?\n",
    "- ‚úÖ Bucket permissions correct?\n",
    "\n",
    "**View logs:**\n",
    "```powershell\n",
    "python scripts/check_pipeline_status.py\n",
    "```\n",
    "\n",
    "Or in GCP Console ‚Üí Vertex AI ‚Üí Pipelines\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Issue 5: Chainlit Won't Start\n",
    "\n",
    "**Symptom:**\n",
    "```\n",
    "chainlit: command not found\n",
    "```\n",
    "\n",
    "**Solution:**\n",
    "```powershell\n",
    "# Use python -m instead\n",
    "python -m chainlit run src/app/main.py -w\n",
    "\n",
    "# Or reinstall\n",
    "pip install chainlit google-auth\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Issue 6: High GCP Costs\n",
    "\n",
    "**Symptom:**\n",
    "Unexpected billing charges\n",
    "\n",
    "**Solution:**\n",
    "```powershell\n",
    "# 1. Check what's deployed\n",
    "python scripts/check_endpoint_status.py\n",
    "\n",
    "# 2. Undeploy immediately\n",
    "python scripts/undeploy_model.py\n",
    "\n",
    "# 3. Verify it's undeployed\n",
    "python scripts/check_endpoint_status.py\n",
    "```\n",
    "\n",
    "**Prevention:**\n",
    "- Always undeploy after testing\n",
    "- Set up billing alerts at $10/month\n",
    "- Check status before leaving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5441de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick diagnostic check\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"üîç QUICK DIAGNOSTIC CHECK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check 1: Python environment\n",
    "print(\"\\n1. Python Environment:\")\n",
    "import sys\n",
    "print(f\"   ‚úÖ Python {sys.version.split()[0]}\")\n",
    "print(f\"   ‚úÖ Virtual env: {sys.prefix}\")\n",
    "\n",
    "# Check 2: GCP Authentication\n",
    "print(\"\\n2. GCP Authentication:\")\n",
    "try:\n",
    "    import google.auth\n",
    "    creds, project = google.auth.default()\n",
    "    print(f\"   ‚úÖ Authenticated\")\n",
    "    print(f\"   ‚úÖ Project: {project}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Not authenticated: {e}\")\n",
    "\n",
    "# Check 3: Required files exist\n",
    "print(\"\\n3. Project Files:\")\n",
    "required_files = [\n",
    "    'src/handler.py',\n",
    "    'src/app/main.py',\n",
    "    'src/constants.py',\n",
    "    'COMBINED_FOOD_DATASET.csv'\n",
    "]\n",
    "\n",
    "for file in required_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"   ‚úÖ {file}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {file} NOT FOUND\")\n",
    "\n",
    "# Check 4: Scripts\n",
    "print(\"\\n4. Deployment Scripts:\")\n",
    "scripts = [\n",
    "    'scripts/deploy_to_endpoint.py',\n",
    "    'scripts/undeploy_model.py',\n",
    "    'scripts/check_endpoint_status.py'\n",
    "]\n",
    "\n",
    "for script in scripts:\n",
    "    if os.path.exists(script):\n",
    "        print(f\"   ‚úÖ {script}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {script} NOT FOUND\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Diagnostic complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8db11e",
   "metadata": {},
   "source": [
    "# üìö Complete Workflow Summary\n",
    "\n",
    "## End-to-End Process\n",
    "\n",
    "### Phase 1: Setup (One-Time)\n",
    "1. Clone repository\n",
    "2. Install dependencies with `uv sync`\n",
    "3. Authenticate with GCP: `gcloud auth application-default login`\n",
    "4. Validate setup: `python scripts/validate_gcp_setup.py`\n",
    "\n",
    "### Phase 2: Training (Optional - Already Done!)\n",
    "1. Upload dataset: `python scripts/upload_dataset.py`\n",
    "2. Run pipeline: `python scripts/pipeline_runner.py`\n",
    "3. Monitor: `python scripts/check_pipeline_status.py`\n",
    "4. **Result:** Model saved to GCS with ID `3561348948692041728`\n",
    "\n",
    "### Phase 3: Deployment (When Needed)\n",
    "1. Deploy model: `python scripts/deploy_to_endpoint.py`\n",
    "2. Wait 5-10 minutes\n",
    "3. Verify: `python scripts/check_endpoint_status.py`\n",
    "4. **Result:** Endpoint `5724492940806455296` serving requests\n",
    "\n",
    "### Phase 4: Usage\n",
    "1. Launch chatbot: `.\\.venv\\Scripts\\chainlit.exe run src/app/main.py -w`\n",
    "2. Open browser: http://localhost:8000\n",
    "3. Chat with your nutrition assistant!\n",
    "\n",
    "### Phase 5: Cleanup (IMPORTANT!)\n",
    "1. Stop chatbot: `Ctrl+C`\n",
    "2. Undeploy model: `python scripts/undeploy_model.py`\n",
    "3. Verify: `python scripts/check_endpoint_status.py`\n",
    "4. **Result:** $0/hour billing\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Key Learnings\n",
    "\n",
    "### What You Built\n",
    "- ‚úÖ Complete MLOps pipeline with Vertex AI\n",
    "- ‚úÖ LoRA fine-tuning with 4-bit quantization\n",
    "- ‚úÖ Production deployment with GPU inference\n",
    "- ‚úÖ Web interface for real users\n",
    "- ‚úÖ Cost management automation\n",
    "\n",
    "### Technologies Mastered\n",
    "- Google Cloud Platform (Vertex AI, GCS)\n",
    "- Kubeflow Pipelines\n",
    "- Hugging Face Transformers\n",
    "- LoRA (Parameter-Efficient Fine-Tuning)\n",
    "- Chainlit (Chat UI framework)\n",
    "- Docker containers\n",
    "- Python packaging\n",
    "\n",
    "### Production Skills\n",
    "- Cost optimization\n",
    "- Monitoring and logging\n",
    "- Error handling\n",
    "- Authentication and security\n",
    "- CI/CD principles\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Additional Resources\n",
    "\n",
    "### Documentation\n",
    "- **Main Guide:** `docs/guides/HOW_TO_LAUNCH.md`\n",
    "- **Quick Reference:** `docs/guides/QUICK_REFERENCE.md`\n",
    "- **French Guide:** `docs/guides/GUIDE_LANCEMENT_FR.md`\n",
    "- **Project Summary:** `docs/references/PROJECT_COMPLETE.md`\n",
    "\n",
    "### GCP Console Links\n",
    "- [Vertex AI Endpoints](https://console.cloud.google.com/vertex-ai/online-prediction/endpoints/5724492940806455296?project=aerobic-polygon-460910-v9)\n",
    "- [Pipeline Runs](https://console.cloud.google.com/vertex-ai/pipelines?project=aerobic-polygon-460910-v9)\n",
    "- [GCS Bucket](https://console.cloud.google.com/storage/browser/llmops_101_europ?project=aerobic-polygon-460910-v9)\n",
    "\n",
    "### External Resources\n",
    "- [Vertex AI Documentation](https://cloud.google.com/vertex-ai/docs)\n",
    "- [Phi-3 Model Card](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)\n",
    "- [LoRA Paper](https://arxiv.org/abs/2106.09685)\n",
    "- [Chainlit Documentation](https://docs.chainlit.io)\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Project Status\n",
    "\n",
    "**‚úÖ COMPLETE & PRODUCTION READY**\n",
    "\n",
    "- Model trained: 2,395 nutrition items\n",
    "- Endpoint created: `5724492940806455296`\n",
    "- Model registered: `3561348948692041728`\n",
    "- Chatbot ready: `src/app/main.py`\n",
    "- Documentation complete: All guides written\n",
    "- Cost management: Deploy/undeploy automation\n",
    "\n",
    "**Current Billing:** $0/hour (model undeployed)\n",
    "\n",
    "---\n",
    "\n",
    "**Made with ‚ù§Ô∏è for Albert School LLM OPS Bootcamp MSC2**  \n",
    "**Completed:** October 21, 2025\n",
    "\n",
    "ü•ó **Your AI Nutrition Assistant is Ready!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
