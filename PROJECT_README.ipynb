{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d27222ca",
   "metadata": {},
   "source": [
    "# 🥗 Nutrition Assistant - Complete MLOps Pipeline\n",
    "\n",
    "**Built for:** Albert School LLM OPS Bootcamp MSC2  \n",
    "**Status:** ✅ Production Ready  \n",
    "**Date Completed:** October 21, 2025  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 Table of Contents\n",
    "\n",
    "1. [Project Overview & Architecture](#1)\n",
    "2. [Environment Setup & Prerequisites](#2)\n",
    "3. [GCP Configuration & Authentication](#3)\n",
    "4. [Data Processing Pipeline](#4)\n",
    "5. [Model Training with Vertex AI](#5)\n",
    "6. [LoRA Fine-Tuning Configuration](#6)\n",
    "7. [Model Deployment to Vertex AI](#7)\n",
    "8. [Custom Handler Implementation](#8)\n",
    "9. [Chainlit Chatbot Interface](#9)\n",
    "10. [Cost Management & Monitoring](#10)\n",
    "11. [Pipeline Components Deep Dive](#11)\n",
    "12. [Troubleshooting Guide](#12)\n",
    "\n",
    "---\n",
    "\n",
    "## What This Project Does\n",
    "\n",
    "This is an **end-to-end MLOps system** that:\n",
    "- ✅ **Transforms** 2,395 nutrition items into conversational format\n",
    "- ✅ **Fine-tunes** Microsoft Phi-3-mini model using LoRA (Low-Rank Adaptation)\n",
    "- ✅ **Deploys** to Vertex AI endpoint with GPU acceleration\n",
    "- ✅ **Serves** via a beautiful Chainlit web interface\n",
    "- ✅ **Manages costs** with deploy/undeploy automation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eec678",
   "metadata": {},
   "source": [
    "# 1. Project Overview & Architecture 🏗️\n",
    "\n",
    "## Complete System Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                         DATA PROCESSING                                  │\n",
    "│  ┌────────────────────────────────────────────────────────────────────┐ │\n",
    "│  │ COMBINED_FOOD_DATASET.csv (2,395 food items)                       │ │\n",
    "│  │ → Transform to conversational Q&A format                           │ │\n",
    "│  │ → Train/Test split                                                 │ │\n",
    "│  │ → Upload to GCS: gs://llmops_101_europ/data/                      │ │\n",
    "│  └────────────────────────────────────────────────────────────────────┘ │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "                                    ↓\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                    VERTEX AI TRAINING PIPELINE                           │\n",
    "│  ┌──────────────┐   ┌──────────────┐   ┌──────────────┐  ┌───────────┐│\n",
    "│  │ Component 1  │ → │ Component 2  │ → │ Component 3  │→ │Component 4││\n",
    "│  │ Transform    │   │ Fine-Tune    │   │ Inference    │  │ Evaluate  ││\n",
    "│  │              │   │ LoRA + 4-bit │   │ Generate     │  │ BLEU/     ││\n",
    "│  │ CSV → JSON   │   │ Phi-3 model  │   │ Predictions  │  │ Rouge     ││\n",
    "│  │              │   │ Tesla T4 GPU │   │              │  │           ││\n",
    "│  └──────────────┘   └──────────────┘   └──────────────┘  └───────────┘│\n",
    "│                                                                          │\n",
    "│  Output: gs://llmops_101_europ/pipeline_root/.../fine_tuned_model/     │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "                                    ↓\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                      MODEL DEPLOYMENT                                    │\n",
    "│  ┌────────────────────────────────────────────────────────────────────┐ │\n",
    "│  │ 1. Register Model → Vertex AI Model Registry                      │ │\n",
    "│  │    Model ID: 3561348948692041728                                  │ │\n",
    "│  │                                                                     │ │\n",
    "│  │ 2. Create Endpoint → nutrition-assistant-endpoint                 │ │\n",
    "│  │    Endpoint ID: 5724492940806455296                               │ │\n",
    "│  │                                                                     │ │\n",
    "│  │ 3. Deploy Model → n1-standard-8 + Tesla T4 GPU                   │ │\n",
    "│  │    Custom Handler: src/handler.py                                 │ │\n",
    "│  │    Container: HuggingFace TGI PyTorch                            │ │\n",
    "│  └────────────────────────────────────────────────────────────────────┘ │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "                                    ↓\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                      CHAINLIT WEB INTERFACE                              │\n",
    "│  ┌────────────────────────────────────────────────────────────────────┐ │\n",
    "│  │ User → Chat UI (localhost:8000)                                   │ │\n",
    "│  │   ↓                                                                │ │\n",
    "│  │ Chainlit App (src/app/main.py)                                    │ │\n",
    "│  │   ↓                                                                │ │\n",
    "│  │ Google Cloud ADC Authentication                                   │ │\n",
    "│  │   ↓                                                                │ │\n",
    "│  │ Vertex AI Endpoint Prediction Request                            │ │\n",
    "│  │   ↓                                                                │ │\n",
    "│  │ Model Response (nutrition advice)                                 │ │\n",
    "│  └────────────────────────────────────────────────────────────────────┘ │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## Key Technologies\n",
    "\n",
    "| Component | Technology | Purpose |\n",
    "|-----------|------------|---------|\n",
    "| **Model** | Microsoft Phi-3-mini-4k-instruct (3.8B params) | Base LLM |\n",
    "| **Fine-Tuning** | LoRA (Low-Rank Adaptation) + 4-bit quantization | Efficient training |\n",
    "| **Pipeline** | Vertex AI + Kubeflow v2.14.6 | Orchestration |\n",
    "| **Compute** | Tesla T4 GPU + n1-standard-8 | Training & serving |\n",
    "| **Storage** | Google Cloud Storage | Data & models |\n",
    "| **Interface** | Chainlit + Python | User chat UI |\n",
    "| **Auth** | Google Cloud ADC | Security |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d418e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load project configuration\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from src.constants import (\n",
    "    GCP_PROJECT_ID, \n",
    "    GCP_REGION, \n",
    "    GCP_BUCKET_NAME,\n",
    "    BASE_MODEL,\n",
    "    TRAINING_CONFIG,\n",
    "    LORA_CONFIG\n",
    ")\n",
    "\n",
    "print(\"📊 PROJECT CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"GCP Project ID:    {GCP_PROJECT_ID}\")\n",
    "print(f\"GCP Region:        {GCP_REGION}\")\n",
    "print(f\"GCS Bucket:        {GCP_BUCKET_NAME}\")\n",
    "print(f\"Base Model:        {BASE_MODEL}\")\n",
    "print(f\"Training Epochs:   {TRAINING_CONFIG['num_train_epochs']}\")\n",
    "print(f\"Batch Size:        {TRAINING_CONFIG['per_device_train_batch_size']}\")\n",
    "print(f\"Learning Rate:     {TRAINING_CONFIG['learning_rate']}\")\n",
    "print(f\"LoRA Rank (r):     {LORA_CONFIG['r']}\")\n",
    "print(f\"LoRA Alpha:        {LORA_CONFIG['lora_alpha']}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ce702c",
   "metadata": {},
   "source": [
    "# 2. Environment Setup & Prerequisites 🔧\n",
    "\n",
    "## Required Software\n",
    "\n",
    "- **Python:** 3.11.6+\n",
    "- **Package Manager:** `uv` (ultra-fast Python package installer)\n",
    "- **Google Cloud SDK:** `gcloud` CLI\n",
    "- **GPU Quota:** NVIDIA Tesla T4 in europe-west2\n",
    "\n",
    "## Environment Validation\n",
    "\n",
    "Run the following checks to ensure your environment is ready:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b2bd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Python version\n",
    "import sys\n",
    "print(f\"✅ Python Version: {sys.version}\")\n",
    "\n",
    "# Check installed packages\n",
    "import subprocess\n",
    "\n",
    "packages_to_check = ['google-cloud-aiplatform', 'chainlit', 'transformers', 'peft', 'bitsandbytes']\n",
    "print(\"\\n📦 Installed Packages:\")\n",
    "for package in packages_to_check:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "        print(f\"  ✅ {package}\")\n",
    "    except ImportError:\n",
    "        print(f\"  ❌ {package} - NOT INSTALLED\")\n",
    "\n",
    "# Check gcloud CLI\n",
    "try:\n",
    "    result = subprocess.run(['gcloud', 'version'], capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"\\n✅ gcloud CLI installed\")\n",
    "        print(result.stdout.split('\\n')[0])\n",
    "    else:\n",
    "        print(\"\\n❌ gcloud CLI not found\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ gcloud CLI not found: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e48c703",
   "metadata": {},
   "source": [
    "# 3. GCP Configuration & Authentication 🔐\n",
    "\n",
    "## Authentication Process\n",
    "\n",
    "To work with Vertex AI and Google Cloud Storage, you need proper authentication:\n",
    "\n",
    "### Method 1: Application Default Credentials (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54e9570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Google Cloud authentication\n",
    "import google.auth\n",
    "from google.cloud import aiplatform, storage\n",
    "\n",
    "try:\n",
    "    # Get default credentials\n",
    "    credentials, project = google.auth.default()\n",
    "    print(f\"✅ Authentication successful!\")\n",
    "    print(f\"📋 Project ID: {project}\")\n",
    "    print(f\"👤 Credentials type: {type(credentials).__name__}\")\n",
    "    \n",
    "    # Initialize Vertex AI\n",
    "    aiplatform.init(\n",
    "        project='aerobic-polygon-460910-v9',\n",
    "        location='europe-west2'\n",
    "    )\n",
    "    print(f\"✅ Vertex AI initialized\")\n",
    "    \n",
    "    # Test GCS access\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket('llmops_101_europ')\n",
    "    if bucket.exists():\n",
    "        print(f\"✅ GCS Bucket accessible: gs://llmops_101_europ\")\n",
    "    else:\n",
    "        print(f\"❌ GCS Bucket not found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Authentication failed: {e}\")\n",
    "    print(\"\\n💡 Run this in terminal:\")\n",
    "    print(\"   gcloud auth application-default login\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f0f88e",
   "metadata": {},
   "source": [
    "# 4. Data Processing Pipeline 📊\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "The dataset consists of 2,395 nutrition items from `COMBINED_FOOD_DATASET.csv`.\n",
    "\n",
    "### Data Transformation Process\n",
    "\n",
    "```\n",
    "Raw CSV → Parse nutritional data → Create Q&A pairs → JSON Lines format\n",
    "```\n",
    "\n",
    "Each food item is converted into a conversation:\n",
    "- **User message:** Question about the food\n",
    "- **Assistant message:** Nutritional information response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4048169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore the dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the CSV dataset\n",
    "df = pd.read_csv('COMBINED_FOOD_DATASET.csv')\n",
    "\n",
    "print(f\"📊 Dataset Statistics:\")\n",
    "print(f\"   Total items: {len(df)}\")\n",
    "print(f\"   Columns: {list(df.columns)}\")\n",
    "print(f\"\\n📋 Sample data:\")\n",
    "print(df.head(3))\n",
    "\n",
    "# Show how data is transformed\n",
    "print(\"\\n\\n🔄 Example Transformation:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load a sample processed conversation\n",
    "try:\n",
    "    with open('data/processed/sample_nutrition_conversations.json', 'r') as f:\n",
    "        sample = json.load(f)\n",
    "    \n",
    "    if isinstance(sample, list) and len(sample) > 0:\n",
    "        example = sample[0]\n",
    "        print(\"INPUT (from CSV):\")\n",
    "        print(f\"  Food: {example.get('food_name', 'N/A')}\")\n",
    "        print(\"\\nOUTPUT (conversational format):\")\n",
    "        print(json.dumps(example.get('messages', []), indent=2))\n",
    "except FileNotFoundError:\n",
    "    print(\"Sample file not found. Run the pipeline to generate it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de00399",
   "metadata": {},
   "source": [
    "# 5. Model Training with Vertex AI 🚀\n",
    "\n",
    "## Pipeline Components\n",
    "\n",
    "The training pipeline consists of 4 components:\n",
    "\n",
    "### Component 1: Data Transformation\n",
    "- **Input:** CSV file from GCS\n",
    "- **Process:** Convert to conversational Q&A format\n",
    "- **Output:** JSON Lines file split into train/test\n",
    "- **Resources:** 4 CPUs, 8GB RAM\n",
    "\n",
    "### Component 2: Fine-Tuning\n",
    "- **Input:** Training data (JSON Lines)\n",
    "- **Process:** Fine-tune Phi-3-mini with LoRA + 4-bit quantization\n",
    "- **Output:** Fine-tuned model saved to GCS\n",
    "- **Resources:** Tesla T4 GPU, 16 CPUs, 50GB RAM\n",
    "- **Duration:** ~30-45 minutes\n",
    "\n",
    "### Component 3: Inference\n",
    "- **Input:** Test data + fine-tuned model\n",
    "- **Process:** Generate predictions on test set\n",
    "- **Output:** Predictions CSV\n",
    "- **Resources:** Tesla T4 GPU, 8 CPUs, 32GB RAM\n",
    "\n",
    "### Component 4: Evaluation\n",
    "- **Input:** Predictions + ground truth\n",
    "- **Process:** Compute BLEU and Rouge scores\n",
    "- **Output:** Metrics JSON\n",
    "- **Resources:** 4 CPUs, 8GB RAM\n",
    "\n",
    "## Pipeline Execution Flow\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Data Transform] --> B[Fine-Tune]\n",
    "    B --> C[Inference]\n",
    "    C --> D[Evaluation]\n",
    "    B -.Model.-> C\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0980fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check latest pipeline run\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project='aerobic-polygon-460910-v9', location='europe-west2')\n",
    "\n",
    "# List recent pipeline runs\n",
    "print(\"📋 Recent Pipeline Runs:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    pipeline_jobs = aiplatform.PipelineJob.list(\n",
    "        filter='display_name:\"nutrition*\"',\n",
    "        order_by='create_time desc'\n",
    "    )\n",
    "    \n",
    "    for i, job in enumerate(list(pipeline_jobs)[:3]):  # Show last 3\n",
    "        print(f\"\\n{i+1}. {job.display_name}\")\n",
    "        print(f\"   State: {job.state.name}\")\n",
    "        print(f\"   Created: {job.create_time}\")\n",
    "        print(f\"   Job ID: {job.name.split('/')[-1]}\")\n",
    "        \n",
    "        if job.state.name == 'PIPELINE_STATE_SUCCEEDED':\n",
    "            print(f\"   ✅ Status: SUCCEEDED\")\n",
    "        elif job.state.name == 'PIPELINE_STATE_RUNNING':\n",
    "            print(f\"   🔄 Status: RUNNING\")\n",
    "        else:\n",
    "            print(f\"   ❌ Status: {job.state.name}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Could not fetch pipeline runs: {e}\")\n",
    "    print(\"\\n💡 Run the pipeline first:\")\n",
    "    print(\"   python scripts/pipeline_runner.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4696c553",
   "metadata": {},
   "source": [
    "# 6. LoRA Fine-Tuning Configuration ⚙️\n",
    "\n",
    "## What is LoRA?\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)** is a parameter-efficient fine-tuning method that:\n",
    "- Freezes the base model weights\n",
    "- Adds small trainable matrices to specific layers\n",
    "- Reduces trainable parameters by ~99.8%\n",
    "- Enables fine-tuning on consumer GPUs\n",
    "\n",
    "## Our LoRA Configuration\n",
    "\n",
    "| Parameter | Value | Purpose |\n",
    "|-----------|-------|---------|\n",
    "| **r (rank)** | 16 | Dimension of low-rank matrices |\n",
    "| **lora_alpha** | 32 | Scaling factor (typically 2×r) |\n",
    "| **lora_dropout** | 0.05 | Dropout for regularization |\n",
    "| **Target Modules** | q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj | Which layers to adapt |\n",
    "\n",
    "## Why 4-bit Quantization?\n",
    "\n",
    "- **Reduces GPU memory:** 3.8B params × 4 bits = ~15GB (vs 60GB for FP16)\n",
    "- **Enables training on T4 GPU:** 16GB VRAM\n",
    "- **Uses NF4 (Normalized Float 4):** Optimal for neural network weights\n",
    "- **Minimal accuracy loss:** <1% with proper configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ee1784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate trainable parameters\n",
    "from src.constants import LORA_CONFIG\n",
    "\n",
    "r = LORA_CONFIG['r']\n",
    "lora_alpha = LORA_CONFIG['lora_alpha']\n",
    "\n",
    "# Phi-3-mini has ~3.8B parameters\n",
    "total_params = 3_800_000_000\n",
    "\n",
    "# Estimate LoRA parameters (simplified calculation)\n",
    "# For each target module: 2 matrices of size (hidden_dim, r)\n",
    "# Phi-3 has hidden_dim=3072, and we target 7 modules per layer across ~32 layers\n",
    "hidden_dim = 3072\n",
    "num_modules_per_layer = 7\n",
    "num_layers = 32\n",
    "\n",
    "lora_params = 2 * hidden_dim * r * num_modules_per_layer * num_layers\n",
    "\n",
    "print(\"📊 LoRA Parameter Analysis\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total Model Parameters:     {total_params:,}\")\n",
    "print(f\"LoRA Trainable Parameters:  {lora_params:,}\")\n",
    "print(f\"Percentage Trainable:       {(lora_params/total_params)*100:.2f}%\")\n",
    "print(f\"\\n💾 Memory Savings:\")\n",
    "print(f\"Full Fine-tuning:           ~60 GB (FP16)\")\n",
    "print(f\"LoRA + 4-bit:               ~15 GB\")\n",
    "print(f\"Reduction:                  75%\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410a8e66",
   "metadata": {},
   "source": [
    "# 7. Model Deployment to Vertex AI 🚢\n",
    "\n",
    "## Deployment Architecture\n",
    "\n",
    "```\n",
    "Model (GCS) → Register to Model Registry → Deploy to Endpoint → Serve Predictions\n",
    "```\n",
    "\n",
    "## Deployment Configuration\n",
    "\n",
    "| Setting | Value | Purpose |\n",
    "|---------|-------|---------|\n",
    "| **Model ID** | 3561348948692041728 | Registered model in Vertex AI |\n",
    "| **Endpoint ID** | 5724492940806455296 | Production endpoint |\n",
    "| **Machine Type** | n1-standard-8 | 8 vCPUs, 30GB RAM |\n",
    "| **Accelerator** | NVIDIA Tesla T4 | GPU for inference |\n",
    "| **Container** | HuggingFace TGI PyTorch | Pre-built serving container |\n",
    "| **Min Replicas** | 1 | Always-on instance |\n",
    "| **Max Replicas** | 1 | No auto-scaling (cost control) |\n",
    "\n",
    "## Deployment Process\n",
    "\n",
    "### Step 1: Register Model\n",
    "The model artifact from the training pipeline is registered to Vertex AI Model Registry with a custom prediction handler.\n",
    "\n",
    "### Step 2: Create/Get Endpoint\n",
    "An endpoint is created (or retrieved if it exists) to serve predictions.\n",
    "\n",
    "### Step 3: Deploy Model\n",
    "The model is deployed to the endpoint with GPU configuration.\n",
    "\n",
    "**⏱️ Deployment time:** 5-10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c60fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check endpoint status\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project='aerobic-polygon-460910-v9', location='europe-west2')\n",
    "\n",
    "ENDPOINT_ID = '5724492940806455296'\n",
    "\n",
    "try:\n",
    "    endpoint = aiplatform.Endpoint(f\"projects/aerobic-polygon-460910-v9/locations/europe-west2/endpoints/{ENDPOINT_ID}\")\n",
    "    \n",
    "    print(\"📍 ENDPOINT STATUS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Endpoint ID:       {ENDPOINT_ID}\")\n",
    "    print(f\"Display Name:      {endpoint.display_name}\")\n",
    "    print(f\"Resource Name:     {endpoint.resource_name}\")\n",
    "    print(f\"Create Time:       {endpoint.create_time}\")\n",
    "    \n",
    "    # Check deployed models\n",
    "    deployed_models = endpoint.list_models()\n",
    "    \n",
    "    if deployed_models:\n",
    "        print(f\"\\n✅ Models Deployed: {len(deployed_models)}\")\n",
    "        for model in deployed_models:\n",
    "            print(f\"\\n   Model ID: {model.id}\")\n",
    "            print(f\"   Display Name: {model.display_name}\")\n",
    "            print(f\"   Machine Type: {model.machine_spec.machine_type}\")\n",
    "            print(f\"   Accelerator: {model.machine_spec.accelerator_type}\")\n",
    "            print(f\"   Accelerator Count: {model.machine_spec.accelerator_count}\")\n",
    "            print(f\"   Traffic: {model.traffic_percentage}%\")\n",
    "    else:\n",
    "        print(f\"\\n⚪ No models currently deployed\")\n",
    "        print(f\"💰 Current cost: $0/hour\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error checking endpoint: {e}\")\n",
    "    print(\"\\n💡 Endpoint may not exist yet. Run:\")\n",
    "    print(\"   python scripts/deploy_to_endpoint.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c778b21e",
   "metadata": {},
   "source": [
    "# 8. 🚀 Launcher - Start/Stop Your Chatbot\n",
    "\n",
    "## ▶️ START THE CHATBOT (3 Steps)\n",
    "\n",
    "### Step 1: Deploy Model to Endpoint\n",
    "\n",
    "```powershell\n",
    "python scripts/deploy_to_endpoint.py\n",
    "```\n",
    "\n",
    "⏱️ **Wait:** 5-10 minutes for deployment to complete\n",
    "\n",
    "### Step 2: Verify Deployment\n",
    "\n",
    "```powershell\n",
    "python scripts/check_endpoint_status.py\n",
    "```\n",
    "\n",
    "✅ **Look for:** \"DEPLOYMENT COMPLETE!\" and Status: \"SERVING\"\n",
    "\n",
    "### Step 3: Launch Web Interface\n",
    "\n",
    "```powershell\n",
    ".\\.venv\\Scripts\\chainlit.exe run src/app/main.py -w\n",
    "```\n",
    "\n",
    "🌐 **Opens at:** http://localhost:8000\n",
    "\n",
    "---\n",
    "\n",
    "## ⏹️ STOP EVERYTHING (Save Money!)\n",
    "\n",
    "### Step 1: Stop Chatbot Interface\n",
    "\n",
    "Press `Ctrl+C` in the terminal running Chainlit\n",
    "\n",
    "### Step 2: Undeploy Model (STOPS BILLING!)\n",
    "\n",
    "```powershell\n",
    "python scripts/undeploy_model.py\n",
    "```\n",
    "\n",
    "💰 **This immediately stops hourly charges!**\n",
    "\n",
    "### Step 3: Verify Model is Undeployed\n",
    "\n",
    "```powershell\n",
    "python scripts/check_endpoint_status.py\n",
    "```\n",
    "\n",
    "✅ **Should show:** \"Status: No models deployed\"\n",
    "\n",
    "---\n",
    "\n",
    "## 🔗 Monitoring Links\n",
    "\n",
    "- **Endpoint Status:** [View in GCP Console](https://console.cloud.google.com/vertex-ai/online-prediction/endpoints/5724492940806455296?project=aerobic-polygon-460910-v9)\n",
    "- **Pipeline History:** [View Training Runs](https://console.cloud.google.com/vertex-ai/pipelines?project=aerobic-polygon-460910-v9)\n",
    "- **Storage Bucket:** [View GCS Files](https://console.cloud.google.com/storage/browser/llmops_101_europ?project=aerobic-polygon-460910-v9)\n",
    "\n",
    "---\n",
    "\n",
    "## 🖱️ Manual Deployment via GCP Console\n",
    "\n",
    "If you prefer to deploy through the web interface:\n",
    "\n",
    "### Settings to Use:\n",
    "\n",
    "| Setting | Value |\n",
    "|---------|-------|\n",
    "| **Model Name** | nutrition-assistant-phi3 |\n",
    "| **Model ID** | 3561348948692041728 |\n",
    "| **Deployed Name** | nutrition-assistant-deployed |\n",
    "| **Machine Type** | n1-standard-8 |\n",
    "| **Accelerator** | NVIDIA_TESLA_T4 |\n",
    "| **Accelerator Count** | 1 |\n",
    "| **Min Nodes** | 1 |\n",
    "| **Max Nodes** | 1 |\n",
    "| **Traffic Split** | 100 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb527ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can run these commands directly from the notebook (not recommended - use terminal instead)\n",
    "# This cell demonstrates what happens when you check status\n",
    "\n",
    "import subprocess\n",
    "\n",
    "print(\"🔍 Checking Endpoint Status...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        ['python', 'scripts/check_endpoint_status.py'],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=30\n",
    "    )\n",
    "    \n",
    "    print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print(\"Errors:\", result.stderr)\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"⏱️ Command timed out\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    \n",
    "print(\"\\n💡 TIP: Run these commands in your terminal for better output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2210e458",
   "metadata": {},
   "source": [
    "# 9. Custom Handler Implementation 🛠️\n",
    "\n",
    "The custom handler (`src/handler.py`) is crucial for deployment. It tells Vertex AI how to:\n",
    "1. Load the fine-tuned model\n",
    "2. Process prediction requests\n",
    "3. Return formatted responses\n",
    "\n",
    "## Handler Structure\n",
    "\n",
    "```python\n",
    "class EndpointHandler:\n",
    "    def __init__(self, model_dir):\n",
    "        # Load the fine-tuned model with LoRA adapters\n",
    "        # Apply 4-bit quantization\n",
    "        # Load tokenizer\n",
    "        \n",
    "    def __call__(self, request):\n",
    "        # Extract input text\n",
    "        # Tokenize\n",
    "        # Generate response\n",
    "        # Return JSON\n",
    "```\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **4-bit Quantization:** Loads model in NF4 format to fit in GPU memory\n",
    "- **LoRA Adapters:** Applies fine-tuned weights on top of base model\n",
    "- **Streaming Support:** Can return responses token-by-token\n",
    "- **Error Handling:** Graceful fallbacks for invalid inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3fa2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the handler implementation\n",
    "with open('src/handler.py', 'r') as f:\n",
    "    handler_code = f.read()\n",
    "    \n",
    "print(\"📄 Custom Handler Code (src/handler.py)\")\n",
    "print(\"=\" * 70)\n",
    "print(handler_code[:1500])  # Show first 1500 characters\n",
    "print(\"\\n... (truncated)\")\n",
    "print(\"\\n💡 Full file: src/handler.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da354af6",
   "metadata": {},
   "source": [
    "# 10. Cost Management & Monitoring 💰\n",
    "\n",
    "## Cost Breakdown\n",
    "\n",
    "| Status | Component | Cost/Hour | Cost/Day (8hrs) | Cost/Month |\n",
    "|--------|-----------|-----------|-----------------|------------|\n",
    "| ⚪ **Undeployed** | Endpoint (empty) | $0.00 | $0.00 | $0.00 |\n",
    "| ⚪ **Undeployed** | GCS Storage | ~$0.001 | ~$0.008 | ~$0.72 |\n",
    "| | **TOTAL UNDEPLOYED** | **$0.001** | **$0.008** | **~$0.72** |\n",
    "| | | | | |\n",
    "| ✅ **Deployed** | n1-standard-8 | ~$0.38 | ~$3.04 | ~$274 |\n",
    "| ✅ **Deployed** | Tesla T4 GPU | ~$0.35 | ~$2.80 | ~$252 |\n",
    "| ✅ **Deployed** | Endpoint overhead | ~$0.02 | ~$0.16 | ~$14 |\n",
    "| | **TOTAL DEPLOYED** | **~$0.75** | **~$6.00** | **~$540** |\n",
    "\n",
    "## Cost Optimization Best Practices\n",
    "\n",
    "1. **✅ Always undeploy after testing**\n",
    "   ```powershell\n",
    "   python scripts/undeploy_model.py\n",
    "   ```\n",
    "\n",
    "2. **✅ Check status before leaving**\n",
    "   ```powershell\n",
    "   python scripts/check_endpoint_status.py\n",
    "   ```\n",
    "\n",
    "3. **✅ Set up billing alerts in GCP Console**\n",
    "   - Go to: Billing → Budgets & alerts\n",
    "   - Set threshold: $10/month\n",
    "   - Get email notifications\n",
    "\n",
    "4. **❌ Don't leave models deployed overnight**\n",
    "   - 8 hours unused = ~$6 wasted\n",
    "   - 1 month unused = ~$540!\n",
    "\n",
    "## Monitoring Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb73801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate estimated costs based on current deployment status\n",
    "from google.cloud import aiplatform\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "aiplatform.init(project='aerobic-polygon-460910-v9', location='europe-west2')\n",
    "\n",
    "ENDPOINT_ID = '5724492940806455296'\n",
    "\n",
    "try:\n",
    "    endpoint = aiplatform.Endpoint(f\"projects/aerobic-polygon-460910-v9/locations/europe-west2/endpoints/{ENDPOINT_ID}\")\n",
    "    deployed_models = endpoint.list_models()\n",
    "    \n",
    "    print(\"💰 CURRENT COST ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if deployed_models:\n",
    "        print(\"⚠️  MODEL IS DEPLOYED - BILLING ACTIVE\")\n",
    "        print(\"\\nHourly Cost Breakdown:\")\n",
    "        print(\"  n1-standard-8 (8 vCPUs, 30GB RAM): $0.38/hour\")\n",
    "        print(\"  Tesla T4 GPU:                      $0.35/hour\")\n",
    "        print(\"  Endpoint overhead:                 $0.02/hour\")\n",
    "        print(\"  \" + \"-\" * 50)\n",
    "        print(\"  TOTAL:                             $0.75/hour\")\n",
    "        \n",
    "        # Estimate time deployed\n",
    "        create_time = endpoint.create_time\n",
    "        if create_time:\n",
    "            hours_deployed = (datetime.now(create_time.tzinfo) - create_time).total_seconds() / 3600\n",
    "            estimated_cost = hours_deployed * 0.75\n",
    "            print(f\"\\nEstimated cost since creation:     ${estimated_cost:.2f}\")\n",
    "            \n",
    "        print(\"\\n⚠️  ACTION REQUIRED: Run `python scripts/undeploy_model.py` to stop billing!\")\n",
    "    else:\n",
    "        print(\"✅ MODEL IS UNDEPLOYED - MINIMAL BILLING\")\n",
    "        print(\"\\nCurrent costs:\")\n",
    "        print(\"  GCS Storage only:                  $0.001/hour\")\n",
    "        print(\"  Daily cost:                        $0.024\")\n",
    "        print(\"  Monthly cost:                      ~$0.72\")\n",
    "        print(\"\\n✅ You're good! No action needed.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Could not check costs: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990c6fb4",
   "metadata": {},
   "source": [
    "# 11. Pipeline Components Deep Dive 🔬\n",
    "\n",
    "## Component Files\n",
    "\n",
    "All pipeline components are in `src/pipeline_components/`:\n",
    "\n",
    "### 1. data_transformation_component.py\n",
    "**Purpose:** Transform CSV to conversational format\n",
    "\n",
    "**Key Functions:**\n",
    "- `transform_data()`: Main transformation logic\n",
    "- `create_conversation()`: Converts food item to Q&A pair\n",
    "- `split_train_test()`: 80/20 split\n",
    "\n",
    "**Inputs:**\n",
    "- `dataset_path`: Path to CSV in GCS\n",
    "\n",
    "**Outputs:**\n",
    "- `train_data`: Training JSON Lines\n",
    "- `test_data`: Test JSON Lines\n",
    "\n",
    "**Resources:**\n",
    "- CPU: 4\n",
    "- RAM: 8GB\n",
    "- Disk: 10GB\n",
    "\n",
    "---\n",
    "\n",
    "### 2. fine_tuning_component.py\n",
    "**Purpose:** Fine-tune Phi-3 with LoRA\n",
    "\n",
    "**Key Functions:**\n",
    "- `load_model()`: Load base model with 4-bit quantization\n",
    "- `apply_lora()`: Add LoRA adapters\n",
    "- `train()`: Training loop with Transformers Trainer\n",
    "\n",
    "**Inputs:**\n",
    "- `train_data`: Training dataset\n",
    "- `base_model_id`: \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "**Outputs:**\n",
    "- `model_dir`: Fine-tuned model in GCS\n",
    "\n",
    "**Resources:**\n",
    "- GPU: Tesla T4 (16GB VRAM)\n",
    "- CPU: 16\n",
    "- RAM: 50GB\n",
    "- Duration: ~30-45 minutes\n",
    "\n",
    "---\n",
    "\n",
    "### 3. inference_component.py\n",
    "**Purpose:** Generate predictions on test set\n",
    "\n",
    "**Key Functions:**\n",
    "- `load_model()`: Load fine-tuned model\n",
    "- `predict()`: Generate responses\n",
    "- `batch_predict()`: Process multiple inputs\n",
    "\n",
    "**Inputs:**\n",
    "- `model_dir`: Fine-tuned model\n",
    "- `test_data`: Test dataset\n",
    "\n",
    "**Outputs:**\n",
    "- `predictions`: CSV with generated responses\n",
    "\n",
    "**Resources:**\n",
    "- GPU: Tesla T4\n",
    "- CPU: 8\n",
    "- RAM: 32GB\n",
    "\n",
    "---\n",
    "\n",
    "### 4. evaluation_component.py\n",
    "**Purpose:** Compute metrics (BLEU, Rouge)\n",
    "\n",
    "**Key Functions:**\n",
    "- `compute_bleu()`: BLEU score calculation\n",
    "- `compute_rouge()`: Rouge score calculation\n",
    "- `aggregate_metrics()`: Summary statistics\n",
    "\n",
    "**Inputs:**\n",
    "- `predictions`: Model predictions\n",
    "- `ground_truth`: Reference answers\n",
    "\n",
    "**Outputs:**\n",
    "- `metrics`: JSON with scores\n",
    "- `per_sample_metrics`: Detailed results\n",
    "\n",
    "**Resources:**\n",
    "- CPU: 4\n",
    "- RAM: 8GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c174187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all component files\n",
    "import os\n",
    "\n",
    "components_dir = 'src/pipeline_components'\n",
    "print(\"📁 Pipeline Component Files:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for filename in os.listdir(components_dir):\n",
    "    if filename.endswith('.py') and not filename.startswith('__'):\n",
    "        filepath = os.path.join(components_dir, filename)\n",
    "        # Get file size\n",
    "        size = os.path.getsize(filepath)\n",
    "        print(f\"\\n✅ {filename}\")\n",
    "        print(f\"   Size: {size:,} bytes ({size//1024} KB)\")\n",
    "        \n",
    "        # Count lines\n",
    "        with open(filepath, 'r') as f:\n",
    "            lines = len(f.readlines())\n",
    "        print(f\"   Lines of code: {lines}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"💡 View full code in: src/pipeline_components/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9081389f",
   "metadata": {},
   "source": [
    "# 12. Troubleshooting Guide 🔧\n",
    "\n",
    "## Common Issues and Solutions\n",
    "\n",
    "### ❌ Issue 1: Authentication Error\n",
    "\n",
    "**Symptom:**\n",
    "```\n",
    "Error: Could not automatically determine credentials\n",
    "```\n",
    "\n",
    "**Solution:**\n",
    "```powershell\n",
    "gcloud auth application-default login\n",
    "```\n",
    "\n",
    "**Why it happens:** Your local credentials expired or weren't set up\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Issue 2: GPU Quota Error\n",
    "\n",
    "**Symptom:**\n",
    "```\n",
    "Quota 'NVIDIA_T4_GPUS' exceeded\n",
    "```\n",
    "\n",
    "**Solution:**\n",
    "1. Go to: [GCP Console → Quotas](https://console.cloud.google.com/iam-admin/quotas)\n",
    "2. Filter: \"Vertex AI\" + \"europe-west2\" + \"NVIDIA T4\"\n",
    "3. Request quota increase to 1\n",
    "4. Wait for approval (usually 1-2 hours)\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Issue 3: Model Not Responding\n",
    "\n",
    "**Symptom:**\n",
    "Chainlit shows errors or no response\n",
    "\n",
    "**Solution:**\n",
    "```powershell\n",
    "# 1. Check endpoint status\n",
    "python scripts/check_endpoint_status.py\n",
    "\n",
    "# 2. If not SERVING, check deployment\n",
    "# Look for errors in GCP Console\n",
    "\n",
    "# 3. If needed, redeploy\n",
    "python scripts/undeploy_model.py\n",
    "python scripts/deploy_to_endpoint.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Issue 4: Pipeline Fails at Fine-Tuning\n",
    "\n",
    "**Symptom:**\n",
    "Pipeline shows \"FAILED\" status on fine-tuning component\n",
    "\n",
    "**Checklist:**\n",
    "- ✅ GPU quota approved?\n",
    "- ✅ Dataset uploaded to GCS?\n",
    "- ✅ Correct region (europe-west2)?\n",
    "- ✅ Bucket permissions correct?\n",
    "\n",
    "**View logs:**\n",
    "```powershell\n",
    "python scripts/check_pipeline_status.py\n",
    "```\n",
    "\n",
    "Or in GCP Console → Vertex AI → Pipelines\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Issue 5: Chainlit Won't Start\n",
    "\n",
    "**Symptom:**\n",
    "```\n",
    "chainlit: command not found\n",
    "```\n",
    "\n",
    "**Solution:**\n",
    "```powershell\n",
    "# Use python -m instead\n",
    "python -m chainlit run src/app/main.py -w\n",
    "\n",
    "# Or reinstall\n",
    "pip install chainlit google-auth\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ❌ Issue 6: High GCP Costs\n",
    "\n",
    "**Symptom:**\n",
    "Unexpected billing charges\n",
    "\n",
    "**Solution:**\n",
    "```powershell\n",
    "# 1. Check what's deployed\n",
    "python scripts/check_endpoint_status.py\n",
    "\n",
    "# 2. Undeploy immediately\n",
    "python scripts/undeploy_model.py\n",
    "\n",
    "# 3. Verify it's undeployed\n",
    "python scripts/check_endpoint_status.py\n",
    "```\n",
    "\n",
    "**Prevention:**\n",
    "- Always undeploy after testing\n",
    "- Set up billing alerts at $10/month\n",
    "- Check status before leaving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5441de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick diagnostic check\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"🔍 QUICK DIAGNOSTIC CHECK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check 1: Python environment\n",
    "print(\"\\n1. Python Environment:\")\n",
    "import sys\n",
    "print(f\"   ✅ Python {sys.version.split()[0]}\")\n",
    "print(f\"   ✅ Virtual env: {sys.prefix}\")\n",
    "\n",
    "# Check 2: GCP Authentication\n",
    "print(\"\\n2. GCP Authentication:\")\n",
    "try:\n",
    "    import google.auth\n",
    "    creds, project = google.auth.default()\n",
    "    print(f\"   ✅ Authenticated\")\n",
    "    print(f\"   ✅ Project: {project}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ❌ Not authenticated: {e}\")\n",
    "\n",
    "# Check 3: Required files exist\n",
    "print(\"\\n3. Project Files:\")\n",
    "required_files = [\n",
    "    'src/handler.py',\n",
    "    'src/app/main.py',\n",
    "    'src/constants.py',\n",
    "    'COMBINED_FOOD_DATASET.csv'\n",
    "]\n",
    "\n",
    "for file in required_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"   ✅ {file}\")\n",
    "    else:\n",
    "        print(f\"   ❌ {file} NOT FOUND\")\n",
    "\n",
    "# Check 4: Scripts\n",
    "print(\"\\n4. Deployment Scripts:\")\n",
    "scripts = [\n",
    "    'scripts/deploy_to_endpoint.py',\n",
    "    'scripts/undeploy_model.py',\n",
    "    'scripts/check_endpoint_status.py'\n",
    "]\n",
    "\n",
    "for script in scripts:\n",
    "    if os.path.exists(script):\n",
    "        print(f\"   ✅ {script}\")\n",
    "    else:\n",
    "        print(f\"   ❌ {script} NOT FOUND\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"✅ Diagnostic complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8db11e",
   "metadata": {},
   "source": [
    "# 📚 Complete Workflow Summary\n",
    "\n",
    "## End-to-End Process\n",
    "\n",
    "### Phase 1: Setup (One-Time)\n",
    "1. Clone repository\n",
    "2. Install dependencies with `uv sync`\n",
    "3. Authenticate with GCP: `gcloud auth application-default login`\n",
    "4. Validate setup: `python scripts/validate_gcp_setup.py`\n",
    "\n",
    "### Phase 2: Training (Optional - Already Done!)\n",
    "1. Upload dataset: `python scripts/upload_dataset.py`\n",
    "2. Run pipeline: `python scripts/pipeline_runner.py`\n",
    "3. Monitor: `python scripts/check_pipeline_status.py`\n",
    "4. **Result:** Model saved to GCS with ID `3561348948692041728`\n",
    "\n",
    "### Phase 3: Deployment (When Needed)\n",
    "1. Deploy model: `python scripts/deploy_to_endpoint.py`\n",
    "2. Wait 5-10 minutes\n",
    "3. Verify: `python scripts/check_endpoint_status.py`\n",
    "4. **Result:** Endpoint `5724492940806455296` serving requests\n",
    "\n",
    "### Phase 4: Usage\n",
    "1. Launch chatbot: `.\\.venv\\Scripts\\chainlit.exe run src/app/main.py -w`\n",
    "2. Open browser: http://localhost:8000\n",
    "3. Chat with your nutrition assistant!\n",
    "\n",
    "### Phase 5: Cleanup (IMPORTANT!)\n",
    "1. Stop chatbot: `Ctrl+C`\n",
    "2. Undeploy model: `python scripts/undeploy_model.py`\n",
    "3. Verify: `python scripts/check_endpoint_status.py`\n",
    "4. **Result:** $0/hour billing\n",
    "\n",
    "---\n",
    "\n",
    "## 🎓 Key Learnings\n",
    "\n",
    "### What You Built\n",
    "- ✅ Complete MLOps pipeline with Vertex AI\n",
    "- ✅ LoRA fine-tuning with 4-bit quantization\n",
    "- ✅ Production deployment with GPU inference\n",
    "- ✅ Web interface for real users\n",
    "- ✅ Cost management automation\n",
    "\n",
    "### Technologies Mastered\n",
    "- Google Cloud Platform (Vertex AI, GCS)\n",
    "- Kubeflow Pipelines\n",
    "- Hugging Face Transformers\n",
    "- LoRA (Parameter-Efficient Fine-Tuning)\n",
    "- Chainlit (Chat UI framework)\n",
    "- Docker containers\n",
    "- Python packaging\n",
    "\n",
    "### Production Skills\n",
    "- Cost optimization\n",
    "- Monitoring and logging\n",
    "- Error handling\n",
    "- Authentication and security\n",
    "- CI/CD principles\n",
    "\n",
    "---\n",
    "\n",
    "## 📖 Additional Resources\n",
    "\n",
    "### Documentation\n",
    "- **Main Guide:** `docs/guides/HOW_TO_LAUNCH.md`\n",
    "- **Quick Reference:** `docs/guides/QUICK_REFERENCE.md`\n",
    "- **French Guide:** `docs/guides/GUIDE_LANCEMENT_FR.md`\n",
    "- **Project Summary:** `docs/references/PROJECT_COMPLETE.md`\n",
    "\n",
    "### GCP Console Links\n",
    "- [Vertex AI Endpoints](https://console.cloud.google.com/vertex-ai/online-prediction/endpoints/5724492940806455296?project=aerobic-polygon-460910-v9)\n",
    "- [Pipeline Runs](https://console.cloud.google.com/vertex-ai/pipelines?project=aerobic-polygon-460910-v9)\n",
    "- [GCS Bucket](https://console.cloud.google.com/storage/browser/llmops_101_europ?project=aerobic-polygon-460910-v9)\n",
    "\n",
    "### External Resources\n",
    "- [Vertex AI Documentation](https://cloud.google.com/vertex-ai/docs)\n",
    "- [Phi-3 Model Card](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)\n",
    "- [LoRA Paper](https://arxiv.org/abs/2106.09685)\n",
    "- [Chainlit Documentation](https://docs.chainlit.io)\n",
    "\n",
    "---\n",
    "\n",
    "## 🎉 Project Status\n",
    "\n",
    "**✅ COMPLETE & PRODUCTION READY**\n",
    "\n",
    "- Model trained: 2,395 nutrition items\n",
    "- Endpoint created: `5724492940806455296`\n",
    "- Model registered: `3561348948692041728`\n",
    "- Chatbot ready: `src/app/main.py`\n",
    "- Documentation complete: All guides written\n",
    "- Cost management: Deploy/undeploy automation\n",
    "\n",
    "**Current Billing:** $0/hour (model undeployed)\n",
    "\n",
    "---\n",
    "\n",
    "**Made with ❤️ for Albert School LLM OPS Bootcamp MSC2**  \n",
    "**Completed:** October 21, 2025\n",
    "\n",
    "🥗 **Your AI Nutrition Assistant is Ready!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
