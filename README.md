# ğŸ¥— Nutrition Assistant - Complete MLOps Pipeline# ğŸ¥— Nutrition Assistant - Complete MLOps Pipeline



**Status:** âœ… Production Ready | **Cost:** $0/hour (when undeployed)  **Status:** âœ… Production Ready | **Cost:** $0/hour (when undeployed)

**Built for:** Albert School LLM OPS Bootcamp MSC2  

**Date Completed:** October 21, 2025End-to-end MLOps pipeline for fine-tuning Microsoft Phi-3 on nutrition data using Vertex AI, with production deployment and web interface.



---## ğŸ¯ Project Overview



## ğŸ“‹ Table of ContentsThis project implements a **complete production ML system** that:

1. **Transforms** nutrition data (2,395 items) into conversational format

1. [Project Overview](#-project-overview)2. **Fine-tunes** Phi-3-mini model with LoRA (Low-Rank Adaptation)

2. [Quick Start](#-quick-start-use-the-chatbot)3. **Deploys** to Vertex AI endpoint with GPU acceleration

3. [Complete Workflow](#-complete-workflow-from-training-to-deployment)4. **Serves** via a beautiful Chainlit chatbot interface

4. [Project Structure](#-project-structure)5. **Manages costs** with easy deploy/undeploy scripts

5. [Configuration](#-configuration)

6. [Cost Management](#-cost-management)## âš¡ Quick Start (Chat with Your Model)

7. [Troubleshooting](#-troubleshooting)

8. [Technical Details](#-technical-details)### Start the Chatbot (3 Commands)

9. [References](#-references)```powershell

python scripts/deploy_to_endpoint.py          # Deploy model (wait 5-10 min)

---python scripts/check_endpoint_status.py        # Verify status = SERVING

python -m chainlit run src/app/main.py -w      # Launch UI at localhost:8000

## ğŸ¯ Project Overview```



This project implements a **complete production MLOps system** for a nutrition chatbot:### Stop Billing (IMPORTANT!)

```powershell

```Ctrl+C                                         # Stop chatbot

End-to-End Pipeline:python scripts/undeploy_model.py               # Stop billing ($0/hour)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”```

â”‚ 1. DATA PROCESSING                                              â”‚

â”‚    â”œâ”€ 2,395 food items from COMBINED_FOOD_DATASET.csv         â”‚ğŸ“– **Full Guide:** See [`HOW_TO_LAUNCH.md`](HOW_TO_LAUNCH.md) or [`QUICK_REFERENCE.md`](QUICK_REFERENCE.md)

â”‚    â””â”€ Converted to conversational Q&A format                   â”‚

â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤## ğŸ—ï¸ Architecture

â”‚ 2. MODEL TRAINING (Vertex AI Pipeline)                         â”‚

â”‚    â”œâ”€ Fine-tune Phi-3-mini-4k-instruct                        â”‚```

â”‚    â”œâ”€ LoRA (Low-Rank Adaptation) - 4-bit quantization         â”‚Complete System:

â”‚    â”œâ”€ Training: 3 epochs, batch size 2, lr=2e-4               â”‚â”œâ”€â”€ Training Pipeline (Vertex AI)

â”‚    â””â”€ Resources: Tesla T4 GPU, 16 CPUs, 50GB RAM              â”‚â”‚   â”œâ”€â”€ Data Transformation â†’ Converts CSV to chat format

â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚   â”œâ”€â”€ Fine-Tuning â†’ Trains Phi-3 with LoRA on GPU

â”‚ 3. MODEL DEPLOYMENT (Vertex AI Endpoint)                       â”‚â”‚   â”œâ”€â”€ Inference â†’ Generates predictions

â”‚    â”œâ”€ Custom handler for Vertex AI format                     â”‚â”‚   â””â”€â”€ Evaluation â†’ Computes Rouge, BLEU metrics

â”‚    â”œâ”€ Endpoint with GPU (n1-standard-8 + Tesla T4)           â”‚â”‚

â”‚    â””â”€ Deploy/Undeploy on demand (cost optimization)           â”‚â””â”€â”€ Production Deployment

â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€ Custom Handler â†’ Processes Vertex AI requests

â”‚ 4. WEB INTERFACE (Chainlit)                                    â”‚    â”œâ”€â”€ Endpoint â†’ Serves model with Tesla T4 GPU

â”‚    â”œâ”€ Beautiful chat interface at localhost:8000              â”‚    â””â”€â”€ Chainlit UI â†’ Beautiful chat interface

â”‚    â”œâ”€ Google Cloud authentication (ADC)                       â”‚```

â”‚    â””â”€ Real-time responses from fine-tuned model               â”‚

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜## ğŸ“‹ Prerequisites

```

- Python 3.11.6+

### What You Get- Google Cloud Project with billing enabled

- Vertex AI API enabled

âœ… **Training Pipeline** - Automated Vertex AI Kubeflow pipeline  - GPU quota (NVIDIA T4) in your region

âœ… **Fine-Tuned Model** - Phi-3 specialized for nutrition questions  - `uv` package manager installed

âœ… **Production Deployment** - GPU-accelerated endpoint  

âœ… **Web Interface** - User-friendly chatbot  ## ğŸš€ Quick Start

âœ… **Cost Management** - Deploy/undeploy scripts  

âœ… **Complete Documentation** - English & French guides  ### 1. Clone and Setup Environment



---```bash

# Clone the repository

## âš¡ Quick Start (Use the Chatbot)git clone <your-repo-url>

cd LLM-OPS

### Prerequisites

# Create virtual environment with uv

```powershelluv sync

# 1. Authenticate with Google Cloud (one time)

gcloud auth application-default login# Activate virtual environment

.venv\Scripts\activate  # Windows

# 2. Verify environment# or

python scripts/check_endpoint_status.pysource .venv/bin/activate  # Linux/Mac

``````



### ğŸš€ Launcher - Start/Stop Your Chatbot

#### â–¶ï¸ Start the Chatbot (3 Steps)

```powershell
# Step 1: Deploy model to endpoint (wait 5-10 minutes)
python scripts/deploy_to_endpoint.py

# Step 2: Verify deployment is complete
python scripts/check_endpoint_status.py
# Look for: "âœ… DEPLOYMENT COMPLETE!" and Status: "SERVING"

# Step 3: Launch web interface
python -m chainlit run src/app/main.py -w
# Opens at: http://localhost:8000
```

**ğŸ”— Monitor Your Deployment:**
- **Endpoint Status:** [View in GCP Console](https://console.cloud.google.com/vertex-ai/online-prediction/endpoints/5724492940806455296?project=aerobic-polygon-460910-v9)
- **Pipeline Runs:** [View Training History](https://console.cloud.google.com/vertex-ai/pipelines?project=aerobic-polygon-460910-v9)
- **Storage:** [View GCS Bucket](https://console.cloud.google.com/storage/browser/llmops_101_europ?project=aerobic-polygon-460910-v9)

#### â¹ï¸ Stop Everything (IMPORTANT - Save Money!)

```powershell
# Step 1: Stop the chatbot interface
# Press Ctrl+C in the terminal running Chainlit

# Step 2: Undeploy model (STOPS BILLING!)
python scripts/undeploy_model.py

# Step 3: Verify model is undeployed
python scripts/check_endpoint_status.py
# Should show: "Status: No models deployed"
```

**ğŸ’° Current Cost:** $0/hour when undeployed | ~$0.50-$1.00/hour when deployed

---

### 2. Configure GCP (One-Time Setup)

```bash
# Authenticate with GCP
gcloud auth login
gcloud config set project aerobic-polygon-460910-v9
gcloud auth application-default login

# Enable required APIs
gcloud services enable aiplatform.googleapis.com
gcloud services enable storage-component.googleapis.com
gcloud services enable compute.googleapis.com
```

### 3. Setup Environment Variables


### Stop Everything (Save Money!)Create a `.env` file in the project root:



```powershell```bash

# 1. Stop chatbot interfaceGCP_PROJECT_ID=aerobic-polygon-460910-v9

Ctrl+C  # in the terminal running ChainlitGCP_REGION=europe-west2

GCP_BUCKET_NAME=llmops_101_europ

# 2. Undeploy model (STOPS BILLING!)```

python scripts/undeploy_model.py

```### 4. Validate GCP Setup



**ğŸ’¡ Pro Tip:** Always run `undeploy_model.py` when done to avoid hourly charges!```bash

python scripts/validate_gcp_setup.py

ğŸ“– **Detailed Guide:** See [`docs/guides/HOW_TO_LAUNCH.md`](docs/guides/HOW_TO_LAUNCH.md)```



---### 5. Upload Dataset to GCS



## ğŸ”„ Complete Workflow (From Training to Deployment)```bash

python scripts/upload_dataset.py

### Phase 1: Environment Setup```



```powershell### 6. Run the Pipeline

# 1. Clone and setup

git clone <your-repo-url>```bash

cd "LLM OPS"# Compile and submit to Vertex AI

python scripts/pipeline_runner.py

# 2. Install dependencies

uv sync# Or compile only (for testing)

.venv\Scripts\activate  # Activate virtual environmentpython scripts/pipeline_runner.py --compile-only

```

# 3. Configure GCP

gcloud auth login### 7. Monitor Pipeline

gcloud config set project aerobic-polygon-460910-v9

gcloud auth application-default login```bash

# Check pipeline status

# 4. Create .env filepython scripts/check_pipeline_status.py

# Copy .env.example to .env and fill in values```

```

Or view in GCP Console:

### Phase 2: Train the Modelhttps://console.cloud.google.com/vertex-ai/pipelines



```powershell## ğŸ“ Project Structure

# 1. Validate GCP setup

python scripts/validate_gcp_setup.py```

LLM-OPS/

# 2. Upload dataset to Google Cloud Storageâ”œâ”€â”€ ğŸ“– Documentation

python scripts/upload_dataset.pyâ”‚   â”œâ”€â”€ HOW_TO_LAUNCH.md              # â­ Main guide - start here!

â”‚   â”œâ”€â”€ QUICK_REFERENCE.md            # Command cheat sheet

# 3. Run training pipeline (takes ~30-45 minutes)â”‚   â”œâ”€â”€ GUIDE_LANCEMENT_FR.md         # French guide

python scripts/pipeline_runner.pyâ”‚   â”œâ”€â”€ PROJECT_COMPLETE.md           # Project summary

â”‚   â””â”€â”€ FINAL_STATUS.md               # Current status

# 4. Monitor pipeline progressâ”‚

python scripts/check_pipeline_status.pyâ”œâ”€â”€ ğŸ”§ Configuration

â”‚   â”œâ”€â”€ .env                          # GCP credentials & endpoint ID

# Or view in GCP Console:â”‚   â”œâ”€â”€ pyproject.toml                # Python dependencies

# https://console.cloud.google.com/vertex-ai/pipelinesâ”‚   â””â”€â”€ COMBINED_FOOD_DATASET.csv     # Training data (2,395 items)

```â”‚

â”œâ”€â”€ ğŸš€ Deployment Scripts

**What Happens During Training:**â”‚   â”œâ”€â”€ scripts/deploy_to_endpoint.py        # Deploy model (start billing)

â”‚   â”œâ”€â”€ scripts/undeploy_model.py            # Stop billing

1. **Data Transformation** - Converts CSV to chat formatâ”‚   â”œâ”€â”€ scripts/check_endpoint_status.py     # Check deployment

2. **Fine-Tuning** - Trains Phi-3 with LoRA on GPUâ”‚   â”œâ”€â”€ scripts/delete_endpoint.py           # Delete endpoint

3. **Inference** - Generates predictions on test setâ”‚   â””â”€â”€ scripts/register_model_with_custom_handler.py

4. **Evaluation** - Computes Rouge and BLEU scoresâ”‚

â”œâ”€â”€ ğŸ“ Training Scripts

**Training Output:** Model saved to `gs://llmops_101_europ/pipeline_root/.../fine_tuned_model`â”‚   â”œâ”€â”€ scripts/validate_gcp_setup.py        # Validate GCP

â”‚   â”œâ”€â”€ scripts/upload_dataset.py            # Upload data

### Phase 3: Deploy to Productionâ”‚   â”œâ”€â”€ scripts/pipeline_runner.py           # Run training pipeline

â”‚   â””â”€â”€ scripts/check_pipeline_status.py     # Monitor training

```powershellâ”‚

# 1. Register model to Vertex AIâ”œâ”€â”€ ğŸ§  Source Code

python scripts/register_model_with_custom_handler.py \â”‚   â”œâ”€â”€ src/handler.py                       # Custom endpoint handler

    --model-uri "gs://llmops_101_europ/pipeline_root/.../fine_tuned_model" \â”‚   â”œâ”€â”€ src/app/main.py                      # Chainlit chatbot UI

    --model-name "nutrition-assistant-phi3"â”‚   â”œâ”€â”€ src/constants.py                     # Configuration

â”‚   â”œâ”€â”€ src/pipeline_components/             # Pipeline components

# 2. Deploy to endpoint (creates endpoint + deploys model)â”‚   â”‚   â”œâ”€â”€ data_transformation_component.py

python scripts/deploy_to_endpoint.pyâ”‚   â”‚   â”œâ”€â”€ fine_tuning_component.py

â”‚   â”‚   â”œâ”€â”€ inference_component.py

# 3. Wait for deployment (5-10 minutes)â”‚   â”‚   â””â”€â”€ evaluation_component.py

python scripts/check_endpoint_status.pyâ”‚   â””â”€â”€ src/pipelines/

```â”‚       â””â”€â”€ model_training_pipeline.py

â”‚

### Phase 4: Use the Chatbotâ””â”€â”€ ğŸ“Š Data & Artifacts

    â””â”€â”€ data/processed/                      # Processed data

```powershell```

# Launch web interface

python -m chainlit run src/app/main.py -w## ğŸ”§ Configuration



# Open browser to: http://localhost:8000### Training Hyperparameters

# Ask questions like:

# - "What are the health benefits of spinach?"Configured in `src/constants.py`:

# - "List high-protein foods"

# - "Which foods are rich in vitamin C?"```python

```TRAINING_CONFIG = {

    "max_seq_length": 512,

### Phase 5: Clean Up    "num_train_epochs": 3,

    "per_device_train_batch_size": 2,

```powershell    "learning_rate": 2e-4,

# Stop chatbot    # ... more parameters

Ctrl+C}

```

# Undeploy model (IMPORTANT - stops billing!)

python scripts/undeploy_model.py### LoRA Configuration



# Optional: Delete endpoint completely```python

python scripts/delete_endpoint.pyLORA_CONFIG = {

```    "r": 16,

    "lora_alpha": 32,

---    "lora_dropout": 0.05,

    "target_modules": ["q_proj", "k_proj", "v_proj", ...],

## ğŸ“ Project Structure}

```

```

LLM OPS/## ğŸ“Š Dataset

â”‚

â”œâ”€â”€ ğŸ“– README.md                          # â­ Main documentation (you are here)- **Source**: COMBINED_FOOD_DATASET.csv

â”œâ”€â”€ ğŸ“‹ .env.example                       # Environment template- **Size**: 2,395 food items

â”œâ”€â”€ ğŸ“Š COMBINED_FOOD_DATASET.csv          # Training data (2,395 items)- **Format**: CSV with nutritional information

â”œâ”€â”€ âš™ï¸  pyproject.toml                    # Python dependencies- **Output**: Conversational Q&A format for Phi-3

â”‚

â”œâ”€â”€ ğŸ“š docs/                              # Documentation### Example Conversation

â”‚   â”œâ”€â”€ guides/                           # User guides

â”‚   â”‚   â”œâ”€â”€ HOW_TO_LAUNCH.md             # Complete launch guide```json

â”‚   â”‚   â”œâ”€â”€ GUIDE_LANCEMENT_FR.md        # French guide{

â”‚   â”‚   â””â”€â”€ QUICK_REFERENCE.md           # Command cheat sheet  "user": "What are the nutritional values for olive oil?",

â”‚   â”œâ”€â”€ references/                       # Technical references  "assistant": "olive oil contains: Calories: 119 kcal, Fat: 13.5g"

â”‚   â”‚   â”œâ”€â”€ PROJECT_COMPLETE.md          # Project summary}

â”‚   â”‚   â”œâ”€â”€ FINAL_STATUS.md              # Current status```

â”‚   â”‚   â”œâ”€â”€ SESSION4_DEPLOYMENT_GUIDE.md

â”‚   â”‚   â”œâ”€â”€ SESSION4_IMPLEMENTATION_SUMMARY.md## ğŸ›ï¸ Pipeline Components

â”‚   â”‚   â””â”€â”€ session[1-4]_practice.md     # Practice exercises

â”‚   â””â”€â”€ archive/                          # Old/deprecated docs### 1. Data Transformation

â”‚- Reads CSV from GCS

â”œâ”€â”€ ğŸš€ scripts/                           # Utility scripts- Converts to chat format

â”‚   â”œâ”€â”€ Training Scripts:- Splits into train/test sets

â”‚   â”‚   â”œâ”€â”€ validate_gcp_setup.py        # Validate GCP connection- Saves as JSON Lines

â”‚   â”‚   â”œâ”€â”€ upload_dataset.py            # Upload data to GCS

â”‚   â”‚   â”œâ”€â”€ pipeline_runner.py           # Run training pipeline### 2. Fine-Tuning

â”‚   â”‚   â””â”€â”€ check_pipeline_status.py     # Monitor training- Loads Phi-3-mini-4k-instruct

â”‚   â”‚- Applies 4-bit quantization

â”‚   â”œâ”€â”€ Deployment Scripts:- Trains with LoRA adapters

â”‚   â”‚   â”œâ”€â”€ register_model_with_custom_handler.py  # Register model- Saves model to GCS

â”‚   â”‚   â”œâ”€â”€ deploy_to_endpoint.py        # Deploy model (starts billing)- **Resources**: T4 GPU, 16 CPU, 50GB RAM

â”‚   â”‚   â”œâ”€â”€ undeploy_model.py            # Undeploy (stops billing)

â”‚   â”‚   â”œâ”€â”€ delete_endpoint.py           # Delete endpoint### 3. Inference

â”‚   â”‚   â””â”€â”€ check_endpoint_status.py     # Check deployment status- Loads fine-tuned model

â”‚   â”‚- Generates predictions

â”‚   â””â”€â”€ Utility Scripts:- Extracts responses

â”‚       â”œâ”€â”€ find_model_uri.py            # Find model URI in GCS- Saves as CSV

â”‚       â”œâ”€â”€ get_console_urls.py          # Generate GCP console URLs- **Resources**: T4 GPU, 8 CPU, 32GB RAM

â”‚       â””â”€â”€ ...other utilities

â”‚### 4. Evaluation

â”œâ”€â”€ ğŸ§  src/                               # Source code- Computes RAGAS metrics

â”‚   â”œâ”€â”€ handler.py                        # Custom Vertex AI handler- RougeScore, BleuScore

â”‚   â”œâ”€â”€ constants.py                      # Configuration constants- Per-sample and aggregated results

â”‚   â”œâ”€â”€ data_processing.py                # Data utilities- **Resources**: 4 CPU, 8GB RAM

â”‚   â”‚

â”‚   â”œâ”€â”€ app/                              # Chainlit web application## ğŸ“ˆ Monitoring & Results

â”‚   â”‚   â””â”€â”€ main.py                       # Chatbot interface

â”‚   â”‚### View Pipeline in GCP Console

â”‚   â”œâ”€â”€ pipelines/                        # Pipeline definitions

â”‚   â”‚   â””â”€â”€ model_training_pipeline.py    # Main training pipeline```

â”‚   â”‚https://console.cloud.google.com/vertex-ai/pipelines?project=aerobic-polygon-460910-v9

â”‚   â””â”€â”€ pipeline_components/              # Pipeline components```

â”‚       â”œâ”€â”€ data_transformation_component.py

â”‚       â”œâ”€â”€ fine_tuning_component.py### Check Artifacts

â”‚       â”œâ”€â”€ inference_component.py

â”‚       â””â”€â”€ evaluation_component.pyAll artifacts are stored in GCS:

â”‚```

â”œâ”€â”€ ğŸ“Š data/                              # Processed datags://llmops_101_europ/

â”‚   â””â”€â”€ processed/â”œâ”€â”€ pipeline_root/          # Pipeline execution artifacts

â”‚       â””â”€â”€ sample_nutrition_conversations.jsonâ”œâ”€â”€ data/                   # Processed datasets

â”‚â””â”€â”€ models/                 # Fine-tuned models

â”œâ”€â”€ ğŸ§ª tests/                             # Test scripts```

â”‚   â”œâ”€â”€ test_data_processing.py

â”‚   â”œâ”€â”€ test_nutrition_model.py### Metrics

â”‚   â”œâ”€â”€ verify_pipeline.py

â”‚   â””â”€â”€ view_examples.pyPipeline outputs:

â”‚- Training loss curves

â”œâ”€â”€ ğŸ“¦ compiled_pipelines/                # Compiled pipeline YAMLs- Evaluation metrics (Rouge, BLEU)

â”‚   â”œâ”€â”€ compiled_nutrition_pipeline.yaml- Per-sample predictions

â”‚   â””â”€â”€ ...pipeline compilation scripts- Model checkpoints

â”‚

â”œâ”€â”€ ğŸ“ˆ outputs/                           # Pipeline outputs & artifacts## ğŸ› ï¸ Troubleshooting

â”‚   â”œâ”€â”€ inference_predictions.csv

â”‚   â”œâ”€â”€ nutrition_evaluation_results.csv### GPU Quota Error

â”‚   â””â”€â”€ pipeline_artifacts/

â”‚If you get a quota error:

â””â”€â”€ ğŸ”§ .chainlit/                         # Chainlit configuration1. Go to GCP Console â†’ IAM & Admin â†’ Quotas

    â”œâ”€â”€ config.toml2. Filter: "Vertex AI", "europe-west2", "NVIDIA T4"

    â””â”€â”€ translations/                     # Multi-language support3. Request quota increase to at least 1

```

### Authentication Error

---

```bash

## âš™ï¸ Configurationgcloud auth application-default login

```

### Environment Variables (.env)

### Pipeline Fails at Fine-Tuning

```bash

# GCP Project ConfigurationCheck:

GCP_PROJECT_ID=aerobic-polygon-460910-v9- GPU quota is approved

GCP_PROJECT_NUMBER=432566588992- Region matches (europe-west2)

GCP_REGION=europe-west2- Dataset is uploaded to GCS

GCP_BUCKET_NAME=llmops_101_europ- Bucket permissions are correct



# Deployment Configuration (set after deployment)## ğŸ”„ Iterating on the Pipeline

GCP_ENDPOINT_ID=5724492940806455296

```### Update hyperparameters:

Edit `src/constants.py` and re-run:

### Training Configuration (`src/constants.py`)```bash

python scripts/pipeline_runner.py

```python```

# Model

BASE_MODEL = "microsoft/Phi-3-mini-4k-instruct"### Test changes locally:

```bash

# Training Hyperparameterspython scripts/pipeline_runner.py --compile-only

TRAINING_CONFIG = {```

    "max_seq_length": 512,

    "num_train_epochs": 3,### Enable caching:

    "per_device_train_batch_size": 2,```bash

    "gradient_accumulation_steps": 4,python scripts/pipeline_runner.py --enable-caching

    "learning_rate": 2e-4,```

    "weight_decay": 0.01,

    "warmup_ratio": 0.1,## ğŸ“š Resources

}

- [Vertex AI Pipelines Documentation](https://cloud.google.com/vertex-ai/docs/pipelines)

# LoRA Configuration- [Phi-3 Model Card](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)

LORA_CONFIG = {- [Kubeflow Pipelines](https://www.kubeflow.org/docs/components/pipelines/)

    "r": 16,- [LoRA Paper](https://arxiv.org/abs/2106.09685)

    "lora_alpha": 32,

    "lora_dropout": 0.05,## ğŸ“ License

    "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", 

                      "gate_proj", "up_proj", "down_proj"],This project is for educational purposes as part of the Albert School LLM OPS Bootcamp.

}

```## ï¿½ Cost Management



---| Status | Cost/Hour | When |

|--------|-----------|------|

## ğŸ’° Cost Management| âœ… Model Undeployed | **$0.00** | Default (safe) |

| âš ï¸ Model Deployed | **$0.50-$1.00** | While testing |

### Current Costs (Model Undeployed)| ğŸ’¾ Storage (GCS) | **~$0.001** | Always (minimal) |



| Component | Status | Cost/Hour | Cost/Month |**ğŸ’¡ Best Practice:** Always run `python scripts/undeploy_model.py` after testing!

|-----------|--------|-----------|------------|

| **Endpoint (empty)** | âœ… Active | $0.00 | $0.00 |## ğŸ“š Key Documentation

| **GCS Storage** | âœ… Active | ~$0.001 | ~$0.72 |

| **Model Deployment** | âšª Undeployed | $0.00 | $0.00 |- **[HOW_TO_LAUNCH.md](HOW_TO_LAUNCH.md)** - Complete launch guide with troubleshooting

| **Total** | | **$0.001/hour** | **~$0.72/month** |- **[QUICK_REFERENCE.md](QUICK_REFERENCE.md)** - One-page command reference

- **[PROJECT_COMPLETE.md](PROJECT_COMPLETE.md)** - Full project summary

### Costs When Deployed- **[FINAL_STATUS.md](FINAL_STATUS.md)** - Current deployment status



| Component | Machine Type | Cost/Hour | Cost/Day (8hrs) |## ğŸ“ What's Included

|-----------|--------------|-----------|-----------------|

| **Endpoint + Model** | n1-standard-8 + Tesla T4 | $0.50-$1.00 | $4.00-$8.00 |âœ… **Training Pipeline** - Vertex AI Kubeflow pipeline with 4 components  

âœ… **Model Deployment** - Production endpoint with GPU (Tesla T4)  

### Cost Optimization Tipsâœ… **Web Interface** - Chainlit chatbot for easy interaction  

âœ… **Cost Management** - Scripts to deploy/undeploy on demand  

1. **Always undeploy after testing**âœ… **Complete Docs** - English & French guides, troubleshooting  

   ```powershellâœ… **Authentication** - Google Cloud Application Default Credentials  

   python scripts/undeploy_model.py

   ```## ğŸ™‹ Support



2. **Check deployment status before leaving**### Quick Checks

   ```powershell```powershell

   python scripts/check_endpoint_status.pypython scripts/check_endpoint_status.py      # Check deployment

   ```python scripts/check_pipeline_status.py      # Check training

```

3. **Delete endpoint if not using for weeks**

   ```powershell### Troubleshooting

   python scripts/delete_endpoint.pySee **[HOW_TO_LAUNCH.md](HOW_TO_LAUNCH.md)** â†’ Section "ğŸ”§ Troubleshooting"

   ```

### GCP Console

---- **Endpoints:** https://console.cloud.google.com/vertex-ai/online-prediction/endpoints/5724492940806455296?project=aerobic-polygon-460910-v9

- **Pipelines:** https://console.cloud.google.com/vertex-ai/pipelines?project=aerobic-polygon-460910-v9

## ğŸ”§ Troubleshooting

---

### Authentication Issues

## ğŸ‰ Project Status

**Problem:** "Error getting access token"

**âœ… COMPLETE & PRODUCTION READY**

```powershell

# Solution: Re-authenticate- Model trained on 2,395 nutrition items

gcloud auth application-default login- Deployed to Vertex AI (currently undeployed to save costs)

```- Chatbot interface ready to launch

- All documentation complete

### Model Not Responding- Cost management implemented



**Problem:** Chatbot shows errors**Current Billing:** $0/hour (model undeployed) ğŸ’°



```powershell---

# 1. Check endpoint status

python scripts/check_endpoint_status.py**Made with â¤ï¸ for Albert School - LLM OPS Bootcamp MSC2**  

**Completed:** October 21, 2025

# 2. Verify model is deployed (Status: "SERVING")

# 3. If not deployed, redeploy**ğŸ¥— Your AI Nutrition Assistant is Ready! Launch it with [`HOW_TO_LAUNCH.md`](HOW_TO_LAUNCH.md)**

python scripts/deploy_to_endpoint.py
```

### Chainlit Won't Start

**Problem:** "chainlit command not found"

```powershell
# Solution: Use python -m
python -m chainlit run src/app/main.py -w

# Or install packages
pip install chainlit google-auth
```

ğŸ“– **More Help:** See [`docs/guides/HOW_TO_LAUNCH.md`](docs/guides/HOW_TO_LAUNCH.md) â†’ Troubleshooting section

---

## ğŸ”¬ Technical Details

### Model Architecture

- **Base Model:** Microsoft Phi-3-mini-4k-instruct (3.8B parameters)
- **Fine-Tuning:** LoRA (Low-Rank Adaptation)
- **Quantization:** 4-bit NF4 with BitsAndBytes
- **Context Length:** 4,096 tokens
- **Trainable Parameters:** ~0.2% of total

### Training Details

- **Dataset:** 2,395 nutrition conversations
- **Training Time:** ~30-45 minutes on Tesla T4
- **GPU Memory:** ~15GB (with 4-bit quantization)
- **Optimizer:** AdamW with warmup

### Deployment Details

- **Container:** Pre-built HuggingFace Inference (PyTorch 2.4, CUDA 12.1)
- **Machine Type:** n1-standard-8 (8 vCPUs, 30GB RAM)
- **Accelerator:** NVIDIA Tesla T4 (16GB VRAM)
- **Replicas:** 1 (auto-scaling ready)

---

## ğŸ“š References

### Documentation

- **User Guides:**
  - [`docs/guides/HOW_TO_LAUNCH.md`](docs/guides/HOW_TO_LAUNCH.md) - Complete launch guide
  - [`docs/guides/QUICK_REFERENCE.md`](docs/guides/QUICK_REFERENCE.md) - Command cheat sheet
  - [`docs/guides/GUIDE_LANCEMENT_FR.md`](docs/guides/GUIDE_LANCEMENT_FR.md) - Guide en franÃ§ais

- **Technical References:**
  - [`docs/references/PROJECT_COMPLETE.md`](docs/references/PROJECT_COMPLETE.md) - Full summary
  - [`docs/references/FINAL_STATUS.md`](docs/references/FINAL_STATUS.md) - Current status

### GCP Console Links

- **Endpoints:** [Your Endpoint](https://console.cloud.google.com/vertex-ai/online-prediction/endpoints/5724492940806455296?project=aerobic-polygon-460910-v9)
- **Pipelines:** [Training Pipelines](https://console.cloud.google.com/vertex-ai/pipelines?project=aerobic-polygon-460910-v9)
- **Storage:** [GCS Bucket](https://console.cloud.google.com/storage/browser/llmops_101_europ?project=aerobic-polygon-460910-v9)

### Quick Commands Reference

```powershell
# Training
python scripts/validate_gcp_setup.py         # Validate setup
python scripts/upload_dataset.py             # Upload data
python scripts/pipeline_runner.py            # Train model
python scripts/check_pipeline_status.py      # Check training

# Deployment
python scripts/deploy_to_endpoint.py         # Deploy (start billing)
python scripts/check_endpoint_status.py      # Check status
python scripts/undeploy_model.py             # Undeploy (stop billing)

# Usage
python -m chainlit run src/app/main.py -w    # Launch chatbot

# Authentication
gcloud auth application-default login        # Re-authenticate
```

---

## ğŸ‰ Project Status

**âœ… COMPLETE & PRODUCTION READY**

- âœ… Model trained on 2,395 nutrition items
- âœ… Deployed to Vertex AI (currently undeployed)
- âœ… Chatbot interface ready
- âœ… Documentation complete
- âœ… Cost management implemented

**Current Billing:** $0/hour (model undeployed) ğŸ’°

---

## ğŸ‘¥ Project Info

- **Built for:** Albert School LLM OPS Bootcamp MSC2
- **Date Completed:** October 21, 2025
- **GCP Project:** aerobic-polygon-460910-v9
- **Region:** europe-west2
- **Endpoint ID:** 5724492940806455296
- **Model ID:** 3561348948692041728

---

**Made with â¤ï¸ for Albert School**

**ğŸ¥— Your AI Nutrition Assistant is Ready!**

Start with: [`docs/guides/HOW_TO_LAUNCH.md`](docs/guides/HOW_TO_LAUNCH.md)
