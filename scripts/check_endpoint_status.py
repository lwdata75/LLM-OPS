"""
V√©rifier le statut d√©taill√© de l'endpoint et des mod√®les d√©ploy√©s.
"""
import os
from google.cloud import aiplatform
from dotenv import load_dotenv

load_dotenv()

PROJECT_ID = os.getenv("GCP_PROJECT_ID", "aerobic-polygon-460910-v9")
REGION = os.getenv("GCP_REGION", "europe-west2")
ENDPOINT_ID = "5724492940806455296"

aiplatform.init(project=PROJECT_ID, location=REGION)

print(f"\n{'='*80}")
print(f"üîç STATUT DE L'ENDPOINT: {ENDPOINT_ID}")
print(f"{'='*80}\n")

try:
    # Charger l'endpoint
    endpoint = aiplatform.Endpoint(f"projects/432566588992/locations/{REGION}/endpoints/{ENDPOINT_ID}")
    
    print(f"üìç Endpoint: {endpoint.display_name}")
    print(f"   ID: {ENDPOINT_ID}")
    print(f"   R√©gion: {REGION}")
    print(f"   Resource: {endpoint.resource_name}\n")
    
    # V√©rifier les mod√®les d√©ploy√©s
    print(f"{'='*80}")
    print("üì¶ MOD√àLES D√âPLOY√âS:")
    print(f"{'='*80}\n")
    
    if endpoint.gca_resource.deployed_models:
        print(f"‚úÖ {len(endpoint.gca_resource.deployed_models)} mod√®le(s) d√©ploy√©(s):\n")
        
        for i, deployed_model in enumerate(endpoint.gca_resource.deployed_models, 1):
            print(f"{i}. Deployed Model:")
            print(f"   Nom: {deployed_model.display_name}")
            print(f"   ID du mod√®le d√©ploy√©: {deployed_model.id}")
            print(f"   Mod√®le source: {deployed_model.model}")
            
            # Extraire l'ID du mod√®le source
            if deployed_model.model:
                source_model_id = deployed_model.model.split('/')[-1].split('@')[0]
                print(f"   ID du mod√®le source: {source_model_id}")
            
            # Traffic split
            if hasattr(deployed_model, 'traffic_split'):
                print(f"   Traffic: {deployed_model.traffic_split}%")
            
            # Machine type
            if hasattr(deployed_model, 'dedicated_resources'):
                if deployed_model.dedicated_resources.machine_spec:
                    print(f"   Machine: {deployed_model.dedicated_resources.machine_spec.machine_type}")
                    if deployed_model.dedicated_resources.machine_spec.accelerator_type:
                        print(f"   GPU: {deployed_model.dedicated_resources.machine_spec.accelerator_type} x {deployed_model.dedicated_resources.machine_spec.accelerator_count}")
                if deployed_model.dedicated_resources.min_replica_count:
                    print(f"   Replicas: {deployed_model.dedicated_resources.min_replica_count}")
            
            print()
            
            # V√©rifier si c'est l'ID mentionn√©
            if deployed_model.id == "3852567797448048640":
                print(f"   üéØ ‚úÖ OUI! C'est l'ID du mod√®le d√©ploy√© que vous avez mentionn√©!")
                print()
        
        print(f"{'='*80}")
        print("‚úÖ D√âPLOIEMENT TERMIN√â!")
        print(f"{'='*80}\n")
        print("Votre endpoint est pr√™t √† recevoir des requ√™tes! üéâ\n")
        print("üîó Console: https://console.cloud.google.com/vertex-ai/online-prediction/endpoints/{ENDPOINT_ID}?project={PROJECT_ID}\n")
        print("Testez maintenant:")
        print("  python scripts/test_endpoint.py")
        print("  OU")
        print("  chainlit run src/app/main.py -w\n")
        
    else:
        print("‚è≥ Aucun mod√®le d√©ploy√© encore.")
        print("   Le d√©ploiement est toujours en cours...")
        print(f"   Surveillez: https://console.cloud.google.com/vertex-ai/online-prediction/endpoints/{ENDPOINT_ID}?project={PROJECT_ID}\n")
    
    print(f"{'='*80}\n")
    
except Exception as e:
    print(f"‚ùå Erreur: {e}\n")
    import traceback
    traceback.print_exc()
